# ============================================================
# Google Colab Notebook: HM-RAG ‚Äî Complete Setup & Run
# ============================================================
# Copy each section between "# ----- CELL X -----" markers
# into a separate Colab cell and run them in order.
# ============================================================

# ----- CELL 1: Clone Repository (shresth branch) -----
import os

if not os.path.exists('/content/HMRAG'):
    !git clone -b shresth https://github.com/ab-2109/HMRAG.git /content/HMRAG
    print("‚úì Repository cloned (shresth branch)")
else:
    print("‚úì Repository already exists")

%cd /content/HMRAG
!git branch
print(f"Working directory: {os.getcwd()}")


# ----- CELL 2: Install System Dependencies + Python Packages -----
import os
os.chdir('/content/HMRAG')

# Install zstd FIRST (required by Ollama installer)
!sudo apt-get update -qq 2>/dev/null
!sudo apt-get install -y -qq zstd 2>/dev/null
print("‚úì zstd installed")

# Do NOT pin numpy ‚Äî let pip resolve it naturally for Python 3.12
# Install from requirements.txt but skip numpy version pin
!pip install -q -r requirements.txt 2>&1 | grep -v "already satisfied" | tail -10

# Ensure critical packages are present
!pip install -q google-search-results langchain-ollama huggingface_hub qwen-vl-utils 2>&1 | tail -5

# Verify key packages
import importlib
for pkg in ['lightrag', 'langchain_ollama', 'serpapi', 'transformers']:
    try:
        importlib.import_module(pkg)
        print(f"  ‚úì {pkg}")
    except ImportError:
        print(f"  ‚ùå {pkg} ‚Äî MISSING")

print("\n‚úì All Python dependencies installed!")


# ----- CELL 3: Patch Source Files (Fixes Dimension Mismatch + Model Size) -----
import os
os.chdir('/content/HMRAG')

# ---- PATCH: retrieval/vector_retrieval.py ----
with open('retrieval/vector_retrieval.py', 'w') as f:
    f.write(r'''from lightrag import LightRAG, QueryParam
from lightrag.llm.ollama import ollama_model_complete, ollama_embed
from lightrag.utils import EmbeddingFunc

from retrieval.base_retrieval import BaseRetrieval


class VectorRetrieval(BaseRetrieval):
    def __init__(self, config):
        self.config = config
        self.mode = getattr(config, 'mode', 'naive')
        self.top_k = getattr(config, 'top_k', 4)
        ollama_host = getattr(config, 'ollama_base_url', 'http://localhost:11434')
        model_name = getattr(config, 'llm_model_name', 'qwen2.5:1.5b')
        working_dir = getattr(config, 'working_dir', './lightrag_workdir')

        self.client = LightRAG(
            working_dir=working_dir,
            llm_model_func=ollama_model_complete,
            llm_model_name=model_name,
            llm_model_max_async=4,
            llm_model_kwargs={"host": ollama_host, "options": {"num_ctx": 4096}},
            embedding_func=EmbeddingFunc(
                embedding_dim=768,
                max_token_size=8192,
                func=lambda texts: ollama_embed.func(
                    texts, embed_model="nomic-embed-text", host=ollama_host
                ),
            ),
        )
        self.results = []

    def find_top_k(self, query):
        try:
            self.results = self.client.query(
                query,
                param=QueryParam(mode="naive", top_k=self.top_k)
            )
        except Exception as e:
            print(f"VectorRetrieval error: {e}")
            self.results = f"Vector retrieval failed: {str(e)}"
        return self.results
''')
print("‚úì Patched retrieval/vector_retrieval.py")

# ---- PATCH: retrieval/graph_retrieval.py ----
with open('retrieval/graph_retrieval.py', 'w') as f:
    f.write(r'''from lightrag import LightRAG, QueryParam
from lightrag.llm.ollama import ollama_model_complete, ollama_embed
from lightrag.utils import EmbeddingFunc

from retrieval.base_retrieval import BaseRetrieval


class GraphRetrieval(BaseRetrieval):
    def __init__(self, config):
        self.config = config
        self.mode = getattr(config, 'mode', 'mix')
        self.top_k = getattr(config, 'top_k', 4)
        ollama_host = getattr(config, 'ollama_base_url', 'http://localhost:11434')
        model_name = getattr(config, 'llm_model_name', 'qwen2.5:1.5b')
        working_dir = getattr(config, 'working_dir', './lightrag_workdir')

        self.client = LightRAG(
            working_dir=working_dir,
            llm_model_func=ollama_model_complete,
            llm_model_name=model_name,
            llm_model_max_async=4,
            llm_model_kwargs={"host": ollama_host, "options": {"num_ctx": 4096}},
            embedding_func=EmbeddingFunc(
                embedding_dim=768,
                max_token_size=8192,
                func=lambda texts: ollama_embed.func(
                    texts, embed_model="nomic-embed-text", host=ollama_host
                ),
            ),
        )
        self.results = []

    def find_top_k(self, query):
        try:
            self.results = self.client.query(
                query,
                param=QueryParam(mode=self.mode, top_k=self.top_k)
            )
        except Exception as e:
            print(f"GraphRetrieval error: {e}")
            self.results = f"Graph retrieval failed: {str(e)}"
        return self.results
''')
print("‚úì Patched retrieval/graph_retrieval.py")

# ---- PATCH: retrieval/web_retrieval.py (SerpAPI only) ----
with open('retrieval/web_retrieval.py', 'w') as f:
    f.write(r'''from langchain_community.utilities import SerpAPIWrapper
from langchain_ollama import OllamaLLM

from retrieval.base_retrieval import BaseRetrieval


class WebRetrieval(BaseRetrieval):
    def __init__(self, config):
        self.config = config
        self.search_engine = "Google"

        serpapi_api_key = getattr(config, 'serpapi_api_key', '')
        self.top_k = getattr(config, 'top_k', 4)
        ollama_base_url = getattr(config, 'ollama_base_url', 'http://localhost:11434')
        web_llm_model = getattr(config, 'web_llm_model_name', 'qwen2.5:1.5b')

        self.client = SerpAPIWrapper(
            serpapi_api_key=serpapi_api_key
        )

        self.llm = OllamaLLM(
            base_url=ollama_base_url,
            model=web_llm_model,
            temperature=0.35,
        )
        self.results = []

    def format_results(self, results):
        """Format search results into readable text."""
        max_results = self.top_k
        processed = []

        if isinstance(results, dict):
            if 'answerBox' in results:
                answer = results['answerBox']
                processed.append(
                    f"Direct answer: {answer.get('answer', '')}\n"
                    f"Source: {answer.get('link', '')}\n"
                )

            if 'organic' in results:
                for item in results['organic'][:max_results]:
                    processed.append(
                        f"[{item.get('title', 'No title')}]\n"
                        f"{item.get('snippet', 'No snippet')}\n"
                        f"Link: {item.get('link', '')}\n"
                    )

        return "\n".join(processed) if processed else "No relevant results found"

    def generation(self, results_with_query):
        """Use Ollama model to generate an answer from search results."""
        try:
            answer = self.llm.invoke(results_with_query)
        except Exception as e:
            print(f"WebRetrieval generation error: {e}")
            answer = f"Web generation failed: {str(e)}"
        return answer

    def find_top_k(self, query):
        try:
            raw_results = self.client.results(query)
            formatted_results = self.format_results(raw_results)
            self.results = self.generation(formatted_results + "\n" + query)
        except Exception as e:
            print(f"WebRetrieval error: {e}")
            self.results = f"Web retrieval failed: {str(e)}"
        return self.results
''')
print("‚úì Patched retrieval/web_retrieval.py (SerpAPI only)")

# ---- PATCH: agents/decompose_agent.py ----
with open('agents/decompose_agent.py', 'w') as f:
    f.write(r'''import os
import re
from typing import List
from langchain_core.prompts import PromptTemplate
from langchain_ollama import OllamaLLM


class DecomposeAgent:
    def __init__(self, config):
        self.config = config
        self.llm = OllamaLLM(
            base_url=getattr(config, 'ollama_base_url', 'http://localhost:11434'),
            model=getattr(config, 'llm_model_name', 'qwen2.5:1.5b'),
            temperature=getattr(config, 'temperature', 0.35),
        )

    def count_intents(self, query: str) -> int:
        prompt = PromptTemplate.from_template(
            "Please calculate how many independent intents are contained in the following query. "
            "Return only an integer:\n{query}\nNumber of intents: "
        )
        max_attempts = 3
        for attempt in range(max_attempts):
            formatted_prompt = prompt.format(query=query)
            response = self.llm.invoke(formatted_prompt)
            try:
                numbers = re.findall(r'\d+', response.strip())
                if numbers:
                    return int(numbers[0])
            except (ValueError, IndexError):
                pass
            if attempt == max_attempts - 1:
                return 1
        return 1

    def decompose(self, query: str) -> List[str]:
        intent_count = self.count_intents(query)
        intent_count = min(intent_count, 3)
        if intent_count > 1:
            return self._split_query(query)
        return [query]

    def _split_query(self, query: str) -> List[str]:
        prompt = PromptTemplate.from_template(
            "Split the following query into multiple independent sub-queries, "
            "separated by '||', without additional explanations:\n{query}\nList of sub-queries: "
        )
        formatted_prompt = prompt.format(query=query)
        response = self.llm.invoke(formatted_prompt)
        sub_queries = [q.strip() for q in response.split("||") if q.strip()]
        if not sub_queries:
            return [query]
        return sub_queries
''')
print("‚úì Patched agents/decompose_agent.py")

# ---- PATCH: agents/summary_agent.py ----
with open('agents/summary_agent.py', 'w') as f:
    f.write(r'''from collections import Counter
from langchain_ollama import OllamaLLM
import re
from transformers import AutoProcessor
import random
import os
import torch

from prompts.base_prompt import build_prompt


class SummaryAgent:
    def __init__(self, config):
        self.config = config
        self.text_llm = OllamaLLM(
            base_url=getattr(config, 'ollama_base_url', 'http://localhost:11434'),
            model=getattr(config, 'llm_model_name', 'qwen2.5:1.5b')
        )
        self.hf_token = getattr(config, 'hf_token', '') or os.environ.get('HF_TOKEN', '')
        self._vision_model = None
        self._processor = None

    def _load_vision_model(self):
        if self._vision_model is None:
            try:
                from transformers import Qwen2_5_VLForConditionalGeneration

                model_name = "Qwen/Qwen2.5-VL-2B-Instruct"

                token_kwargs = {}
                if self.hf_token:
                    token_kwargs['token'] = self.hf_token

                self._processor = AutoProcessor.from_pretrained(
                    model_name, use_fast=True, **token_kwargs
                )
                self._vision_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    **token_kwargs
                )
            except Exception as e:
                print(f"Warning: Could not load vision model: {e}")
                self._vision_model = None
                self._processor = None

    def summarize(self, problems, shot_qids, qid, cur_ans):
        problem = problems[qid]
        question = problem['question']
        choices = problem["choices"]
        answer = problem['answer']
        image = problem.get('image', '')
        caption = problem.get('caption', '')
        split = problem.get("split", "test")

        most_ans = self.get_most_common_answer(cur_ans)

        if len(most_ans) == 1:
            prediction = self.get_result(most_ans[0])
            pred_idx = self.get_pred_idx(prediction, choices, self.config.options)
        else:
            if image and image == "image.png":
                image_path = os.path.join(self.config.image_root, split, qid, image)
            else:
                image_path = ""

            output_text = cur_ans[0] if len(cur_ans) > 0 else ""
            output_graph = cur_ans[1] if len(cur_ans) > 1 else ""
            output_web = cur_ans[2] if len(cur_ans) > 2 else ""

            output = self.refine(output_text, output_graph, output_web,
                                 problems, shot_qids, qid, self.config, image_path)
            if output is None:
                output = "FAILED"
            print(f"output: {output}")

            ans_fusion = self.get_result(output)
            pred_idx = self.get_pred_idx(ans_fusion, choices, self.config.options)
        return pred_idx, cur_ans

    def get_most_common_answer(self, res):
        if not res:
            return []
        counter = Counter(res)
        max_count = max(counter.values())
        most_common_values = [item for item, count in counter.items() if count == max_count]
        return most_common_values

    def refine(self, output_text, output_graph, output_web, problems, shot_qids, qid, args, image_path):
        prompt = build_prompt(problems, shot_qids, qid, args)
        prompt = f"{prompt} The answer is A, B, C, D, E or FAILED. \n BECAUSE: "

        if not image_path:
            output = self.text_llm.invoke(prompt)
        else:
            output = self.qwen_reasoning(prompt, image_path)
            if output:
                print(f"**** output: {output}")
                output = self.text_llm.invoke(
                    f"{output[0]} Summary the above information with format "
                    f"'Answer: The answer is A, B, C, D, E or FAILED.\n BECAUSE: '"
                )
            else:
                output = self.text_llm.invoke(prompt)
        return output

    def get_result(self, output):
        pattern = re.compile(r'The answer is ([A-E])')
        res = pattern.findall(output)
        if len(res) == 1:
            answer = res[0]
        else:
            answer = "FAILED"
        return answer

    def get_pred_idx(self, prediction, choices, options):
        if prediction in options[:len(choices)]:
            return options.index(prediction)
        else:
            return random.choice(range(len(choices)))

    def qwen_reasoning(self, prompt, image_path):
        self._load_vision_model()
        if self._vision_model is None or self._processor is None:
            print("Warning: Vision model not available, falling back to text-only.")
            return None

        try:
            from qwen_vl_utils import process_vision_info
        except ImportError:
            print("Warning: qwen_vl_utils not installed, falling back to text-only.")
            return None

        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "image": image_path,
                    },
                    {"type": "text", "text": prompt},
                ],
            }
        ]

        text = self._processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self._processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )

        device = next(self._vision_model.parameters()).device
        inputs = {k: v.to(device) if hasattr(v, 'to') else v for k, v in inputs.items()}

        generated_ids = self._vision_model.generate(**inputs, max_new_tokens=512)
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs['input_ids'], generated_ids)
        ]
        output_text = self._processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        return output_text
''')
print("‚úì Patched agents/summary_agent.py")

# ---- PATCH: YAML configs ----
for yaml_file in ['configs/decompose_agent.yaml', 'configs/multi_retrieval_agents.yaml']:
    if os.path.exists(yaml_file):
        with open(yaml_file, 'r') as f:
            content = f.read()
        if 'qwen2.5:7b' in content:
            content = content.replace('qwen2.5:7b', 'qwen2.5:1.5b')
            with open(yaml_file, 'w') as f:
                f.write(content)
            print(f"‚úì Patched {yaml_file}")
        else:
            print(f"‚úì {yaml_file} already correct")

# Clean stale workdir
!rm -rf ./lightrag_workdir 2>/dev/null

print("\n" + "=" * 60)
print("‚úì ALL PATCHES APPLIED!")
print("  embedding_dim=768, ollama_embed.func, num_ctx=4096")
print("  Text: qwen2.5:1.5b | Vision: Qwen2.5-VL-2B-Instruct")
print("  Web search: SerpAPI only | HF token: supported")
print("=" * 60)


# ----- CELL 4: Install Ollama + Pull Models -----
import subprocess
import time
import os

# Install zstd (Ollama requires it ‚Äî ensure it's there)
!sudo apt-get install -y -qq zstd 2>/dev/null

# Install Ollama
print("Installing Ollama...")
!curl -fsSL https://ollama.com/install.sh | sh 2>&1 | tail -3

# Find ollama binary
ollama_path = None
for path in ['/usr/local/bin/ollama', '/usr/bin/ollama']:
    if os.path.exists(path):
        ollama_path = path
        break

if not ollama_path:
    result = subprocess.run(['which', 'ollama'], capture_output=True, text=True)
    ollama_path = result.stdout.strip() if result.stdout.strip() else None

if not ollama_path:
    raise RuntimeError(
        "‚ùå Ollama binary not found after install!\n"
        "Try: Runtime ‚Üí Restart runtime, then rerun from Cell 1."
    )

print(f"‚úì Ollama found at: {ollama_path}")

# Kill any existing ollama processes
subprocess.run(['pkill', '-9', '-f', 'ollama'], stderr=subprocess.DEVNULL)
time.sleep(3)

# Start Ollama server in background
print("\nStarting Ollama server...")
ollama_proc = subprocess.Popen(
    [ollama_path, 'serve'],
    stdout=subprocess.DEVNULL,
    stderr=subprocess.DEVNULL
)
time.sleep(10)

# Verify server is up
for attempt in range(5):
    try:
        result = subprocess.run(
            ['curl', '-s', '-o', '/dev/null', '-w', '%{http_code}', 'http://localhost:11434/api/tags'],
            capture_output=True, text=True, timeout=5
        )
        if result.stdout.strip() == '200':
            print("‚úì Ollama server is running!")
            break
    except Exception:
        pass
    print(f"  Waiting for server... (attempt {attempt+1}/5)")
    time.sleep(5)
else:
    print("‚ö†Ô∏è  Server may not be fully ready, continuing anyway...")

# Pull models
print("\nPulling qwen2.5:1.5b (~1GB)...")
!{ollama_path} pull qwen2.5:1.5b

print("\nPulling nomic-embed-text (~270MB)...")
!{ollama_path} pull nomic-embed-text

print("\n‚úì Ollama setup complete! Available models:")
!{ollama_path} list


# ----- CELL 5: Configure API Keys -----
import os

# ===== METHOD 1: Colab Secrets (recommended) =====
# Go to left sidebar ‚Üí üîë icon ‚Üí Add:
#   SERPAPI_API_KEY = your_serpapi_key
#   HF_TOKEN = your_hf_token (optional)

SERPAPI_API_KEY = ""
HF_TOKEN = ""

try:
    from google.colab import userdata
    try:
        SERPAPI_API_KEY = userdata.get('SERPAPI_API_KEY')
        print("‚úì SERPAPI_API_KEY loaded from Colab Secrets")
    except Exception:
        pass
    try:
        HF_TOKEN = userdata.get('HF_TOKEN')
        if HF_TOKEN:
            print("‚úì HF_TOKEN loaded from Colab Secrets")
    except Exception:
        pass
except ImportError:
    pass

# ===== METHOD 2: Paste directly (if not using Secrets) =====
if not SERPAPI_API_KEY:
    SERPAPI_API_KEY = ""  # <-- PASTE YOUR SERPAPI KEY HERE
    if SERPAPI_API_KEY:
        print("‚úì SERPAPI_API_KEY set manually")
    else:
        print("‚ö†Ô∏è  SERPAPI_API_KEY not set! Web search will fail.")
        print("   Get a free key at: https://serpapi.com")

if not HF_TOKEN:
    HF_TOKEN = ""  # <-- PASTE YOUR HF TOKEN HERE (optional)
    if HF_TOKEN:
        print("‚úì HF_TOKEN set manually")
    else:
        print("‚ÑπÔ∏è  HF_TOKEN not set (optional, needed for gated models)")

os.environ['SERPAPI_API_KEY'] = SERPAPI_API_KEY or ''
os.environ['HF_TOKEN'] = HF_TOKEN or ''

print(f"\nSerpAPI: {'‚úì SET' if SERPAPI_API_KEY else '‚úó NOT SET'}")
print(f"HF Token: {'‚úì SET' if HF_TOKEN else '‚úó NOT SET (optional)'}")


# ----- CELL 6: Download ScienceQA Dataset -----
import os
os.chdir('/content/HMRAG')

os.makedirs('dataset', exist_ok=True)

if not os.path.exists('dataset/ScienceQA'):
    print("Cloning ScienceQA repository...")
    !git clone https://github.com/lupantech/ScienceQA dataset/ScienceQA 2>&1 | tail -3
else:
    print("‚úì ScienceQA already cloned")

if os.path.exists('dataset/ScienceQA/tools/download.sh'):
    print("\nDownloading dataset files (may take 5-10 min)...")
    os.chdir('dataset/ScienceQA')
    !bash tools/download.sh 2>&1 | tail -10
    os.chdir('/content/HMRAG')

# Find data root
print("\nSearching for problems.json...")
data_root = None
for candidate in [
    'dataset/ScienceQA/data/scienceqa',
    'dataset/ScienceQA/data',
]:
    if os.path.exists(os.path.join(candidate, 'problems.json')):
        data_root = candidate
        break

if data_root:
    print(f"‚úì Data root found: {data_root}")
    !ls -lh {data_root}/ | head -10
else:
    print("‚ùå problems.json not found. Searching...")
    !find dataset/ -name "problems.json" 2>/dev/null
    print("\nSet data_root manually in the run cell below.")


# ----- CELL 7: Verify Setup -----
import subprocess
import os

os.chdir('/content/HMRAG')

print("=" * 60)
print("VERIFICATION")
print("=" * 60)

# 1. Ollama server
print("\n[1/4] Ollama server...")
try:
    r = subprocess.run(['curl', '-s', 'http://localhost:11434/api/tags'],
                       capture_output=True, text=True, timeout=5)
    if r.returncode == 0:
        print("  ‚úì Running")
    else:
        print("  ‚ö†Ô∏è  Not responding ‚Äî rerun Cell 4")
except Exception:
    print("  ‚ö†Ô∏è  Not responding ‚Äî rerun Cell 4")

# 2. Models
print("\n[2/4] Ollama models...")
!ollama list 2>/dev/null

# 3. Source files
print("\n[3/4] Source files...")
for f in [
    'main.py',
    'retrieval/vector_retrieval.py',
    'retrieval/graph_retrieval.py',
    'retrieval/web_retrieval.py',
    'retrieval/base_retrieval.py',
    'agents/decompose_agent.py',
    'agents/summary_agent.py',
    'agents/multi_retrieval_agents.py',
    'prompts/base_prompt.py',
]:
    status = "‚úì" if os.path.exists(f) else "‚ùå MISSING"
    print(f"  {status} {f}")

# 4. Imports
print("\n[4/4] Python imports...")
import sys
if '/content/HMRAG' not in sys.path:
    sys.path.insert(0, '/content/HMRAG')
try:
    from langchain_community.utilities import SerpAPIWrapper
    print("  ‚úì SerpAPIWrapper")
except ImportError as e:
    print(f"  ‚ùå {e}")
try:
    from langchain_ollama import OllamaLLM
    print("  ‚úì OllamaLLM")
except ImportError as e:
    print(f"  ‚ùå {e}")
try:
    from lightrag import LightRAG
    from lightrag.llm.ollama import ollama_model_complete, ollama_embed
    print("  ‚úì LightRAG + ollama functions")
except ImportError as e:
    print(f"  ‚ùå {e}")

print("\n" + "=" * 60)
print("‚úì READY TO RUN")
print("=" * 60)


# ----- CELL 8: Run Small Test (5 examples) -----
import os
os.chdir('/content/HMRAG')

# CRITICAL: always delete old workdir to avoid dimension mismatch
!rm -rf ./lightrag_workdir
!mkdir -p outputs

# Auto-detect data_root
data_root = None
for candidate in [
    './dataset/ScienceQA/data/scienceqa',
    './dataset/ScienceQA/data',
]:
    if os.path.exists(os.path.join(candidate, 'problems.json')):
        data_root = candidate
        break

if not data_root:
    print("‚ùå Cannot find problems.json ‚Äî set data_root manually below:")
    print("   data_root = '/content/HMRAG/dataset/ScienceQA/data/scienceqa'")
else:
    serpapi_key = os.environ.get('SERPAPI_API_KEY', '')
    hf_token = os.environ.get('HF_TOKEN', '')

    cmd = (
        f'python3 main.py'
        f' --data_root "{data_root}"'
        f' --image_root "./dataset/ScienceQA/data/scienceqa"'
        f' --output_root "./outputs"'
        f' --working_dir "./lightrag_workdir"'
        f' --serpapi_api_key "{serpapi_key}"'
        f' --llm_model_name "qwen2.5:1.5b"'
        f' --web_llm_model_name "qwen2.5:1.5b"'
        f' --test_split test'
        f' --test_number 5'
        f' --shot_number 0'
        f' --label test_run'
        f' --save_every 5'
    )
    if hf_token:
        cmd += f' --hf_token "{hf_token}"'

    print(f"Command:\n{cmd}\n")
    print("=" * 60)
    !{cmd}


# ----- CELL 9: Run Full Inference -----
import os
os.chdir('/content/HMRAG')

# CRITICAL: always delete old workdir
!rm -rf ./lightrag_workdir
!mkdir -p outputs

data_root = None
for candidate in [
    './dataset/ScienceQA/data/scienceqa',
    './dataset/ScienceQA/data',
]:
    if os.path.exists(os.path.join(candidate, 'problems.json')):
        data_root = candidate
        break

if not data_root:
    print("‚ùå Cannot find problems.json")
else:
    serpapi_key = os.environ.get('SERPAPI_API_KEY', '')
    hf_token = os.environ.get('HF_TOKEN', '')

    cmd = (
        f'python3 main.py'
        f' --data_root "{data_root}"'
        f' --image_root "./dataset/ScienceQA/data/scienceqa"'
        f' --output_root "./outputs"'
        f' --working_dir "./lightrag_workdir"'
        f' --serpapi_api_key "{serpapi_key}"'
        f' --llm_model_name "qwen2.5:1.5b"'
        f' --web_llm_model_name "qwen2.5:1.5b"'
        f' --test_split test'
        f' --shot_number 2'
        f' --label full_run'
        f' --save_every 50'
        f' --use_caption'
    )
    if hf_token:
        cmd += f' --hf_token "{hf_token}"'

    print(f"Command:\n{cmd}\n")
    print("This will take 30min - 2hrs depending on test set size...")
    print("=" * 60)
    !{cmd}


# ----- CELL 10: View Results -----
import os
import json
import glob

os.chdir('/content/HMRAG')

output_files = sorted(glob.glob('outputs/*.json'))
if output_files:
    for fpath in output_files:
        fname = os.path.basename(fpath)
        size_kb = os.path.getsize(fpath) / 1024
        with open(fpath, 'r') as f:
            results = json.load(f)
        print(f"\n{'='*50}")
        print(f"File: {fname} ({size_kb:.1f} KB)")
        print(f"Total predictions: {len(results)}")
        print("First 5 results:")
        for qid, answer in list(results.items())[:5]:
            print(f"  Question {qid}: {answer}")
else:
    print("No output files found. Run inference first (Cell 8 or 9).")


# ----- CELL 11: Download Results -----
from google.colab import files
import os

os.chdir('/content/HMRAG')

!zip -r -q outputs.zip outputs/ 2>/dev/null
if os.path.exists('outputs.zip'):
    size_mb = os.path.getsize('outputs.zip') / (1024 * 1024)
    print(f"‚úì outputs.zip ({size_mb:.2f} MB)")
    files.download('outputs.zip')
else:
    print("‚ùå No outputs to download")