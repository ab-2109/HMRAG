{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a266db7e",
   "metadata": {},
   "source": [
    "## Step 1: Clone the Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ab-2109/HMRAG.git\n",
    "%cd HMRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95236abd",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047520fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Handle dependency conflicts by installing core packages first\n",
    "import os\n",
    "\n",
    "print(\"Installing dependencies (this may take a few minutes)...\")\n",
    "\n",
    "# Install numpy first to avoid conflicts\n",
    "!pip install -q numpy==1.26.4\n",
    "\n",
    "# Install core dependencies with compatible versions\n",
    "!pip install -q --no-deps lightrag-hku\n",
    "!pip install -q langchain langchain-community langchain-core\n",
    "!pip install -q transformers torch tqdm ollama\n",
    "!pip install -q google-search-results\n",
    "!pip install -q networkx aiohttp tenacity tiktoken\n",
    "\n",
    "# Install optional vision dependencies if needed\n",
    "# Uncomment if you need vision model support (Qwen2.5-VL)\n",
    "# !pip install -q qwen_vl_utils opencv-python\n",
    "\n",
    "print(\"✓ Dependencies installed successfully!\")\n",
    "print(\"Note: Some dependency warnings are normal and won't affect functionality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f79de87",
   "metadata": {},
   "source": [
    "## Step 3: Install and Setup Ollama (Required for LLM)\n",
    "Note: Ollama is required for this system. On Colab, follow these steps to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cf7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama on Colab\n",
    "print(\"Installing zstd (required for Ollama)...\")\n",
    "!sudo apt-get update -qq\n",
    "!sudo apt-get install -y -qq zstd\n",
    "\n",
    "print(\"Installing Ollama...\")\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Verify installation\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Check if ollama is installed\n",
    "try:\n",
    "    result = subprocess.run(['which', 'ollama'], capture_output=True, text=True)\n",
    "    ollama_path = result.stdout.strip()\n",
    "    \n",
    "    if not ollama_path:\n",
    "        print(\"⚠️ Ollama not found in PATH. Trying common locations...\")\n",
    "        if os.path.exists('/usr/local/bin/ollama'):\n",
    "            ollama_path = '/usr/local/bin/ollama'\n",
    "        elif os.path.exists('/usr/bin/ollama'):\n",
    "            ollama_path = '/usr/bin/ollama'\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Ollama binary not found\")\n",
    "    \n",
    "    print(f\"✓ Ollama found at: {ollama_path}\")\n",
    "    \n",
    "    # Start Ollama server in background\n",
    "    print(\"Starting Ollama server...\")\n",
    "    ollama_process = subprocess.Popen(\n",
    "        [ollama_path, 'serve'],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    time.sleep(10)  # Wait for server to start\n",
    "    \n",
    "    # Pull required models\n",
    "    print(\"Pulling qwen2.5:7b model (this may take 5-10 minutes)...\")\n",
    "    !{ollama_path} pull qwen2.5:7b\n",
    "    \n",
    "    print(\"Pulling nomic-embed-text model...\")\n",
    "    !{ollama_path} pull nomic-embed-text\n",
    "    \n",
    "    print(\"✓ Ollama setup complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error setting up Ollama: {e}\")\n",
    "    print(\"You may need to restart the runtime and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e68520b",
   "metadata": {},
   "source": [
    "## Step 4: Setup API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291fed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API keys here\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Store secrets in Colab's secret manager (left sidebar -> Key icon)\n",
    "# Then access them like this:\n",
    "try:\n",
    "    SERPAPI_API_KEY = userdata.get('SERPAPI_API_KEY')\n",
    "except:\n",
    "    # Or set directly (not recommended for production)\n",
    "    SERPAPI_API_KEY = \"your-serpapi-key-here\"\n",
    "\n",
    "print(\"API keys configured!\")\n",
    "print(\"Note: OpenAI API key is not required. System uses Ollama for LLM inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4ac9f",
   "metadata": {},
   "source": [
    "## Step 5: Download ScienceQA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the ScienceQA dataset\n",
    "!bash dataset/download_ScienceQA.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c2de56",
   "metadata": {},
   "source": [
    "## Step 6: Create Required Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b65d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output and working directories\n",
    "!mkdir -p outputs\n",
    "!mkdir -p lightrag_workdir\n",
    "\n",
    "# Check dataset structure\n",
    "!ls -la dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba6c07c",
   "metadata": {},
   "source": [
    "## Step 7: Run Inference (Small Test)\n",
    "Start with a small test run (5 examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5188dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on a small subset first to test\n",
    "!python3 main.py \\\n",
    "    --data_root ./dataset/ScienceQA/data \\\n",
    "    --image_root ./dataset/ScienceQA/images \\\n",
    "    --output_root ./outputs \\\n",
    "    --caption_file ./dataset/ScienceQA/captions.json \\\n",
    "    --working_dir ./lightrag_workdir \\\n",
    "    --serpapi_api_key \"$SERPAPI_API_KEY\" \\\n",
    "    --test_split test \\\n",
    "    --test_number 5 \\\n",
    "    --shot_number 0 \\\n",
    "    --label test_run \\\n",
    "    --save_every 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4337dc11",
   "metadata": {},
   "source": [
    "## Step 8: Run Full Inference\n",
    "After testing, run on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6099f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full inference run\n",
    "!python3 main.py \\\n",
    "    --data_root ./dataset/ScienceQA/data \\\n",
    "    --image_root ./dataset/ScienceQA/images \\\n",
    "    --output_root ./outputs \\\n",
    "    --caption_file ./dataset/ScienceQA/captions.json \\\n",
    "    --working_dir ./lightrag_workdir \\\n",
    "    --serpapi_api_key \"$SERPAPI_API_KEY\" \\\n",
    "    --test_split test \\\n",
    "    --shot_number 2 \\\n",
    "    --label full_run \\\n",
    "    --save_every 50 \\\n",
    "    --use_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec69152",
   "metadata": {},
   "source": [
    "## Step 9: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0efdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View output files\n",
    "!ls -lh outputs/\n",
    "\n",
    "# Load and display results\n",
    "import json\n",
    "\n",
    "with open('outputs/test_run_test.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(f\"Total results: {len(results)}\")\n",
    "print(\"\\nSample results:\")\n",
    "for qid, answer in list(results.items())[:5]:\n",
    "    print(f\"Question ID: {qid}, Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964324d",
   "metadata": {},
   "source": [
    "## Alternative: Use OpenAI Models Directly\n",
    "If Ollama setup is difficult, modify the agents to use OpenAI API directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d9c70",
   "metadata": {},
   "source": [
    "## Download Results to Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f74e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "from google.colab import files\n",
    "\n",
    "# Download the results file\n",
    "files.download('outputs/test_run_test.json')\n",
    "\n",
    "# Or zip and download all outputs\n",
    "!zip -r outputs.zip outputs/\n",
    "files.download('outputs.zip')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
