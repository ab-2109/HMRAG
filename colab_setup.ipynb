{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a266db7e",
   "metadata": {},
   "source": [
    "## Step 1: Clone the Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ab-2109/HMRAG.git\n",
    "%cd HMRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c87fe60",
   "metadata": {},
   "source": [
    "## Step 1b: Patch Retrieval Files (Fix embedding dimension)\n",
    "The GitHub repo has hardcoded `embedding_dim=1024`, but `nomic-embed-text` outputs 768 dims. This cell patches the files directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c66f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CRITICAL FIX: Patch retrieval files to use correct embedding dimension (768)\n",
    "# nomic-embed-text outputs 768-dim vectors, NOT 1024\n",
    "# =============================================================================\n",
    "import os\n",
    "\n",
    "files_to_patch = [\n",
    "    'retrieval/vector_retrieval.py',\n",
    "    'retrieval/graph_retrieval.py',\n",
    "]\n",
    "\n",
    "for filepath in files_to_patch:\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        if 'embedding_dim=1024' in content:\n",
    "            content = content.replace('embedding_dim=1024', 'embedding_dim=768')\n",
    "            with open(filepath, 'w') as f:\n",
    "                f.write(content)\n",
    "            print(f\"✓ Patched {filepath}: embedding_dim 1024 → 768\")\n",
    "        elif 'embedding_dim=768' in content:\n",
    "            print(f\"✓ {filepath} already correct (embedding_dim=768)\")\n",
    "        else:\n",
    "            print(f\"⚠️ {filepath}: embedding_dim not found, check manually\")\n",
    "    else:\n",
    "        print(f\"❌ {filepath} not found!\")\n",
    "\n",
    "# Also fix the deprecated Ollama import in web_retrieval.py\n",
    "web_retrieval_path = 'retrieval/web_retrieval.py'\n",
    "if os.path.exists(web_retrieval_path):\n",
    "    with open(web_retrieval_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    if 'from langchain_community.llms.ollama import Ollama' in content:\n",
    "        content = content.replace(\n",
    "            'from langchain_community.llms.ollama import Ollama',\n",
    "            'from langchain_ollama import OllamaLLM as Ollama'\n",
    "        )\n",
    "        with open(web_retrieval_path, 'w') as f:\n",
    "            f.write(content)\n",
    "        print(f\"✓ Patched {web_retrieval_path}: Updated Ollama import\")\n",
    "\n",
    "# Fix deprecated import in summary_agent.py too\n",
    "summary_path = 'agents/summary_agent.py'\n",
    "if os.path.exists(summary_path):\n",
    "    with open(summary_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    if 'from langchain_community.llms.ollama import Ollama' in content:\n",
    "        content = content.replace(\n",
    "            'from langchain_community.llms.ollama import Ollama',\n",
    "            'from langchain_ollama import OllamaLLM as Ollama'\n",
    "        )\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(content)\n",
    "        print(f\"✓ Patched {summary_path}: Updated Ollama import\")\n",
    "\n",
    "# Clean up any leftover working directory from previous runs\n",
    "!rm -rf ./lightrag_workdir\n",
    "print(\"\\n✓ Cleaned lightrag_workdir\")\n",
    "print(\"✓ All patches applied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95236abd",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047520fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Handle dependency conflicts by installing core packages first\n",
    "import os\n",
    "\n",
    "print(\"Installing dependencies (this may take a few minutes)...\")\n",
    "\n",
    "# Install numpy first to avoid conflicts\n",
    "!pip install -q numpy==1.26.4\n",
    "\n",
    "# Install core dependencies with compatible versions\n",
    "!pip install -q --no-deps lightrag-hku\n",
    "!pip install -q langchain langchain-community langchain-core langchain-ollama\n",
    "!pip install -q transformers torch tqdm ollama\n",
    "!pip install -q google-search-results\n",
    "!pip install -q networkx aiohttp tenacity tiktoken\n",
    "\n",
    "# Install optional vision dependencies if needed\n",
    "# Uncomment if you need vision model support (Qwen2.5-VL)\n",
    "# !pip install -q qwen_vl_utils opencv-python\n",
    "\n",
    "print(\"✓ Dependencies installed successfully!\")\n",
    "print(\"Note: Some dependency warnings are normal and won't affect functionality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f79de87",
   "metadata": {},
   "source": [
    "## Step 3: Install and Setup Ollama (Required for LLM)\n",
    "Note: Ollama is required for this system. On Colab, follow these steps to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cf7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing zstd (required for Ollama)...\")\n",
    "!sudo apt-get update -qq\n",
    "!sudo apt-get install -y -qq zstd\n",
    "\n",
    "print(\"Installing Ollama...\")\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Verify installation\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Check if ollama is installed# Install Ollama on Colab\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['which', 'ollama'], capture_output=True, text=True)\n",
    "    ollama_path = result.stdout.strip()\n",
    "    \n",
    "    if not ollama_path:\n",
    "        print(\"⚠️ Ollama not found in PATH. Trying common locations...\")\n",
    "        if os.path.exists('/usr/local/bin/ollama'):\n",
    "            ollama_path = '/usr/local/bin/ollama'\n",
    "        elif os.path.exists('/usr/bin/ollama'):\n",
    "            ollama_path = '/usr/bin/ollama'\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Ollama binary not found\")\n",
    "    \n",
    "    print(f\"✓ Ollama found at: {ollama_path}\")\n",
    "    \n",
    "    # Start Ollama server in background\n",
    "    print(\"Starting Ollama server...\")\n",
    "    ollama_process = subprocess.Popen(\n",
    "        [ollama_path, 'serve'],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    time.sleep(10)  # Wait for server to start\n",
    "    \n",
    "    # Pull required models\n",
    "    print(\"Pulling qwen2.5:7b model (this may take 5-10 minutes)...\")\n",
    "    !{ollama_path} pull qwen2.5:7b\n",
    "    \n",
    "    print(\"Pulling nomic-embed-text model...\")\n",
    "    !{ollama_path} pull nomic-embed-text\n",
    "    \n",
    "    print(\"✓ Ollama setup complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error setting up Ollama: {e}\")\n",
    "    print(\"You may need to restart the runtime and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e68520b",
   "metadata": {},
   "source": [
    "## Step 4: Setup API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291fed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API keys here\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Store secrets in Colab's secret manager (left sidebar -> Key icon)\n",
    "# Then access them like this:\n",
    "try:\n",
    "    SERPAPI_API_KEY = userdata.get('SERPAPI_API_KEY')\n",
    "except:\n",
    "    # Or set directly (not recommended for production)\n",
    "    SERPAPI_API_KEY = \"your-serpapi-key-here\"\n",
    "\n",
    "print(\"API keys configured!\")\n",
    "print(\"Note: OpenAI API key is not required. System uses Ollama for LLM inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bedd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Ollama is running (run this before inference if you get connection errors)\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "try:\n",
    "    # Check if Ollama server is responding\n",
    "    result = subprocess.run(['curl', '-s', 'http://localhost:11434/api/tags'], \n",
    "                          capture_output=True, text=True, timeout=5)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"✓ Ollama server is running!\")\n",
    "    else:\n",
    "        raise Exception(\"Ollama not responding\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"⚠️ Ollama server not running. Restarting...\")\n",
    "    \n",
    "    # Find ollama binary\n",
    "    result = subprocess.run(['which', 'ollama'], capture_output=True, text=True)\n",
    "    ollama_path = result.stdout.strip()\n",
    "    \n",
    "    if not ollama_path:\n",
    "        if os.path.exists('/usr/local/bin/ollama'):\n",
    "            ollama_path = '/usr/local/bin/ollama'\n",
    "        elif os.path.exists('/usr/bin/ollama'):\n",
    "            ollama_path = '/usr/bin/ollama'\n",
    "    \n",
    "    # Kill any existing ollama processes\n",
    "    subprocess.run(['pkill', 'ollama'], stderr=subprocess.DEVNULL)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Start Ollama server in background\n",
    "    ollama_process = subprocess.Popen(\n",
    "        [ollama_path, 'serve'],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    \n",
    "    print(\"Waiting for Ollama to start...\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Verify it's running\n",
    "    result = subprocess.run(['curl', '-s', 'http://localhost:11434/api/tags'], \n",
    "                          capture_output=True, text=True, timeout=5)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"✓ Ollama server restarted successfully!\")\n",
    "    else:\n",
    "        print(\"❌ Failed to start Ollama server. You may need to restart the runtime.\")\n",
    "        \n",
    "# Show available models\n",
    "print(\"\\nAvailable models:\")\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4ac9f",
   "metadata": {},
   "source": [
    "## Step 5: Download ScienceQA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the ScienceQA dataset\n",
    "import os\n",
    "\n",
    "# Make sure we're in the HMRAG directory\n",
    "if not os.path.exists('main.py'):\n",
    "    %cd /content/HMRAG\n",
    "\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "# Create dataset directory if it doesn't exist\n",
    "if not os.path.exists('dataset'):\n",
    "    os.makedirs('dataset')\n",
    "\n",
    "%cd dataset\n",
    "\n",
    "# Clone the ScienceQA repository\n",
    "if not os.path.exists('ScienceQA'):\n",
    "    print(\"Cloning ScienceQA repository...\")\n",
    "    !git clone https://github.com/lupantech/ScienceQA\n",
    "    \n",
    "    if os.path.exists('ScienceQA'):\n",
    "        print(\"✓ Repository cloned successfully\")\n",
    "        %cd ScienceQA\n",
    "        \n",
    "        # Download the actual data\n",
    "        if os.path.exists('tools/download.sh'):\n",
    "            print(\"Downloading dataset files (this may take several minutes)...\")\n",
    "            !bash tools/download.sh\n",
    "        else:\n",
    "            print(\"Warning: download.sh not found, trying alternative...\")\n",
    "            !mkdir -p data\n",
    "            # You may need to manually download the dataset\n",
    "            \n",
    "        %cd ..\n",
    "    else:\n",
    "        print(\"❌ Failed to clone repository\")\n",
    "else:\n",
    "    print(\"✓ ScienceQA directory already exists\")\n",
    "\n",
    "# Go back to HMRAG directory\n",
    "%cd ..\n",
    "\n",
    "# Verify the dataset structure and files\n",
    "print(\"\\n=== Checking dataset structure ===\")\n",
    "print(\"HMRAG directory contents:\")\n",
    "!ls -la\n",
    "\n",
    "print(\"\\nDataset directory contents:\")\n",
    "if os.path.exists('dataset/ScienceQA'):\n",
    "    !ls -la dataset/ScienceQA/\n",
    "    \n",
    "    print(\"\\nData directory contents:\")\n",
    "    if os.path.exists('dataset/ScienceQA/data'):\n",
    "        !ls -la dataset/ScienceQA/data/\n",
    "        \n",
    "        # Check for required files\n",
    "        required_files = ['problems.json', 'pid_splits.json']\n",
    "        for file in required_files:\n",
    "            file_path = f'dataset/ScienceQA/data/{file}'\n",
    "            if os.path.exists(file_path):\n",
    "                print(f\"✓ Found: {file}\")\n",
    "            else:\n",
    "                print(f\"❌ Missing: {file}\")\n",
    "    else:\n",
    "        print(\"❌ data directory not found!\")\n",
    "else:\n",
    "    print(\"❌ ScienceQA directory not found!\")\n",
    "\n",
    "print(\"\\n=== Setup complete. Check for any missing files above ===.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c2de56",
   "metadata": {},
   "source": [
    "## Step 6: Create Required Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b65d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output and working directories\n",
    "!mkdir -p outputs\n",
    "!mkdir -p lightrag_workdir\n",
    "\n",
    "# Check dataset structure\n",
    "!ls -la dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba6c07c",
   "metadata": {},
   "source": [
    "## Step 7: Run Inference (Small Test)\n",
    "Start with a small test run (5 examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5188dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on a small subset first to test\n",
    "\n",
    "# CRITICAL: Clean up previous LightRAG working directory to avoid dimension mismatch errors\n",
    "# If previous runs (failed or successful) used different dimensions, the database will be corrupted.\n",
    "!rm -rf ./lightrag_workdir\n",
    "\n",
    "!python3 main.py \\\n",
    "    --data_root ./dataset/ScienceQA/data \\\n",
    "    --image_root ./dataset/ScienceQA/images \\\n",
    "    --output_root ./outputs \\\n",
    "    --caption_file ./dataset/ScienceQA/data/captions.json \\\n",
    "    --working_dir ./lightrag_workdir \\\n",
    "    --serpapi_api_key \"$SERPAPI_API_KEY\" \\\n",
    "    --test_split test \\\n",
    "    --test_number 5 \\\n",
    "    --shot_number 0 \\\n",
    "    --label test_run \\\n",
    "    --save_every 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4337dc11",
   "metadata": {},
   "source": [
    "## Step 8: Run Full Inference\n",
    "After testing, run on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6099f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up previous LightRAG working directory to avoid dimension mismatch errors\n",
    "# (Old runs might have created a DB with default 1024 dims, which conflicts with nomic-embed-text's 768 dims)\n",
    "!rm -rf ./lightrag_workdir\n",
    "\n",
    "# Full inference run\n",
    "!python3 main.py \\\n",
    "    --data_root ./dataset/ScienceQA/data \\\n",
    "    --image_root ./dataset/ScienceQA/images \\\n",
    "    --output_root ./outputs \\\n",
    "    --caption_file ./dataset/ScienceQA/data/captions.json \\\n",
    "    --working_dir ./lightrag_workdir \\\n",
    "    --serpapi_api_key \"$SERPAPI_API_KEY\" \\\n",
    "    --test_split test \\\n",
    "    --shot_number 2 \\\n",
    "    --label full_run \\\n",
    "    --save_every 50 \\\n",
    "    --use_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec69152",
   "metadata": {},
   "source": [
    "## Step 9: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0efdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View output files\n",
    "!ls -lh outputs/\n",
    "\n",
    "# Load and display results\n",
    "import json\n",
    "\n",
    "with open('outputs/test_run_test.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(f\"Total results: {len(results)}\")\n",
    "print(\"\\nSample results:\")\n",
    "for qid, answer in list(results.items())[:5]:\n",
    "    print(f\"Question ID: {qid}, Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964324d",
   "metadata": {},
   "source": [
    "## Alternative: Use OpenAI Models Directly\n",
    "If Ollama setup is difficult, modify the agents to use OpenAI API directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d9c70",
   "metadata": {},
   "source": [
    "## Download Results to Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f74e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "from google.colab import files\n",
    "\n",
    "# Download the results file\n",
    "files.download('outputs/test_run_test.json')\n",
    "\n",
    "# Or zip and download all outputs\n",
    "!zip -r outputs.zip outputs/\n",
    "files.download('outputs.zip')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
