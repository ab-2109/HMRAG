{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba9d365",
   "metadata": {},
   "source": [
    "# HM-RAG: Hierarchical Multi-Agent Multimodal RAG ‚Äî Google Colab Setup\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Clones the repo\n",
    "2. Installs all dependencies (from `requirements.txt`)\n",
    "3. Installs & starts Ollama, pulls `qwen2.5:1.5b` + `nomic-embed-text`\n",
    "4. Patches source files to fix the **embedding dimension mismatch** (768 vs 1024)\n",
    "5. Downloads ScienceQA dataset\n",
    "6. Configures API keys (Serper API for web search, optional HF token)\n",
    "7. Runs inference\n",
    "\n",
    "**Model used:** `qwen2.5:1.5b` (text) + `Qwen/Qwen2.5-VL-2B-Instruct` (vision, only if images present)\n",
    "**Web search:** Serper API only (requires a free key from https://serper.dev)\n",
    "**LLM backend:** All LLM calls (including LightRAG) go through locally hosted Ollama ‚Äî no OpenAI API key needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73399052",
   "metadata": {},
   "source": [
    "## Step 1: Clone the Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone only if not already cloned\n",
    "if not os.path.exists('/content/HMRAG'):\n",
    "    !git clone https://github.com/ab-2109/HMRAG.git /content/HMRAG\n",
    "    print(\"‚úì Repository cloned\")\n",
    "else:\n",
    "    print(\"‚úì Repository already exists\")\n",
    "\n",
    "%cd /content/HMRAG\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c6b3b1",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies\n",
    "Installs all packages from `requirements.txt`. This handles Serper API (`requests`), LightRAG, LangChain, transformers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60cc31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/HMRAG')\n",
    "\n",
    "print(\"Installing dependencies from requirements.txt (this may take a few minutes)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Install numpy first to avoid conflicts\n",
    "!pip install -q numpy==1.26.4\n",
    "\n",
    "# Install from the repo's requirements.txt\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Ensure critical packages are installed (in case requirements.txt missed any)\n",
    "!pip install -q requests langchain-ollama huggingface_hub\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì All dependencies installed successfully!\")\n",
    "print(\"Note: Some dependency warnings are normal and won't affect functionality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7677c2",
   "metadata": {},
   "source": [
    "## Step 3: Patch Source Files (Critical Fixes)\n",
    "This cell patches the cloned source files to:\n",
    "1. **Fix embedding dimension mismatch**: `nomic-embed-text` outputs 768 dims, but LightRAG's `ollama_embed` decorator defaults to 1024. We use `ollama_embed.func` (unwrapped) and set `embedding_dim=768`.\n",
    "2. **Use `qwen2.5:1.5b`** everywhere instead of `qwen2.5:7b` (fits in Colab GPU memory).\n",
    "3. **LightRAG uses Ollama** (`ollama_model_complete`) ‚Äî no GPT-4o-mini / OpenAI API key needed.\n",
    "4. **Reduce `num_ctx` to 4096** (the 1.5B model can't handle 65536).\n",
    "5. **Web search uses Serper API** (`requests.post` to `https://google.serper.dev/search`).\n",
    "6. **Use `Qwen/Qwen2-VL-2B-Instruct`** for vision (preprocessing image captioning & decision agent).\n",
    "7. **Add HF token support** for downloading gated models.\n",
    "8. **Fix device handling** in the vision model to avoid dimension mismatches on Colab.\n",
    "9. **Create preprocessing module** ‚Äî Phase 1 (Section 3.1): image‚Üítext via VLM, assemble documents, index into LightRAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b9bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/HMRAG')\n",
    "\n",
    "# =============================================================================\n",
    "# PATCH 1: retrieval/vector_retrieval.py\n",
    "# =============================================================================\n",
    "with open('retrieval/vector_retrieval.py', 'w') as f:\n",
    "    f.write('''\"\"\"Vector-based Retrieval Agent (HM-RAG Layer 2, Section 3.3.1).\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import Any\n",
    "\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm.ollama import ollama_model_complete, ollama_embed\n",
    "from lightrag.utils import EmbeddingFunc\n",
    "\n",
    "from retrieval.base_retrieval import BaseRetrieval\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "_SEED_DOCUMENT = (\n",
    "    \"Science is the systematic study of the natural world through observation \"\n",
    "    \"and experimentation. Key branches include physics, chemistry, biology, \"\n",
    "    \"earth science, and astronomy.\"\n",
    ")\n",
    "\n",
    "\n",
    "class VectorRetrieval(BaseRetrieval):\n",
    "    MODE = \"naive\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.mode = self.MODE\n",
    "        self._initialised = False\n",
    "        ollama_host = getattr(config, 'ollama_base_url', 'http://localhost:11434')\n",
    "        model_name = getattr(config, 'llm_model_name', 'qwen2.5:1.5b')\n",
    "        working_dir = getattr(config, 'working_dir', './lightrag_workdir')\n",
    "\n",
    "        self.client = LightRAG(\n",
    "            working_dir=working_dir,\n",
    "            llm_model_func=ollama_model_complete,\n",
    "            llm_model_name=model_name,\n",
    "            llm_model_max_async=4,\n",
    "            llm_model_kwargs={\"host\": ollama_host, \"options\": {\"num_ctx\": 4096}},\n",
    "            embedding_func=EmbeddingFunc(\n",
    "                embedding_dim=768,\n",
    "                max_token_size=8192,\n",
    "                func=lambda texts: ollama_embed.func(\n",
    "                    texts, embed_model=\"nomic-embed-text\", host=ollama_host\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "        logger.info('VectorRetrieval initialised | mode=%s | dir=%s', self.mode, working_dir)\n",
    "\n",
    "    def _ensure_initialised(self):\n",
    "        if self._initialised:\n",
    "            return\n",
    "        try:\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            loop = asyncio.get_event_loop()\n",
    "            loop.run_until_complete(self.client.ainsert(_SEED_DOCUMENT))\n",
    "            logger.info('VectorRetrieval: seed document inserted (fallback)')\n",
    "        except Exception as e:\n",
    "            logger.debug('VectorRetrieval: seed insert skipped: %s', e)\n",
    "        self._initialised = True\n",
    "\n",
    "    def find_top_k(self, query):\n",
    "        self._ensure_initialised()\n",
    "        try:\n",
    "            result = self.client.query(\n",
    "                query,\n",
    "                param=QueryParam(mode=self.MODE, top_k=self.top_k)\n",
    "            )\n",
    "            return str(result) if result else ''\n",
    "        except Exception as e:\n",
    "            logger.error('VectorRetrieval error: %s', e)\n",
    "            return f'Vector retrieval failed: {e}'\n",
    "''')\n",
    "print(\"‚úì Patched retrieval/vector_retrieval.py\")\n",
    "\n",
    "# =============================================================================\n",
    "# PATCH 2: retrieval/graph_retrieval.py\n",
    "# =============================================================================\n",
    "with open('retrieval/graph_retrieval.py', 'w') as f:\n",
    "    f.write('''\"\"\"Graph-based Retrieval Agent (HM-RAG Layer 2, Section 3.3.2).\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import Any\n",
    "\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm.ollama import ollama_model_complete, ollama_embed\n",
    "from lightrag.utils import EmbeddingFunc\n",
    "\n",
    "from retrieval.base_retrieval import BaseRetrieval\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "_SEED_DOCUMENT = (\n",
    "    \"Science is the systematic study of the natural world through observation \"\n",
    "    \"and experimentation. Key branches include physics, chemistry, biology, \"\n",
    "    \"earth science, and astronomy.\"\n",
    ")\n",
    "\n",
    "\n",
    "class GraphRetrieval(BaseRetrieval):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.mode = getattr(config, 'graph_search_mode', getattr(config, 'mode', 'mix'))\n",
    "        self._initialised = False\n",
    "        ollama_host = getattr(config, 'ollama_base_url', 'http://localhost:11434')\n",
    "        model_name = getattr(config, 'llm_model_name', 'qwen2.5:1.5b')\n",
    "        working_dir = getattr(config, 'working_dir', './lightrag_workdir')\n",
    "\n",
    "        self.client = LightRAG(\n",
    "            working_dir=working_dir,\n",
    "            llm_model_func=ollama_model_complete,\n",
    "            llm_model_name=model_name,\n",
    "            llm_model_max_async=4,\n",
    "            llm_model_kwargs={\"host\": ollama_host, \"options\": {\"num_ctx\": 4096}},\n",
    "            embedding_func=EmbeddingFunc(\n",
    "                embedding_dim=768,\n",
    "                max_token_size=8192,\n",
    "                func=lambda texts: ollama_embed.func(\n",
    "                    texts, embed_model=\"nomic-embed-text\", host=ollama_host\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "        logger.info('GraphRetrieval initialised | mode=%s | dir=%s', self.mode, working_dir)\n",
    "\n",
    "    def _ensure_initialised(self):\n",
    "        if self._initialised:\n",
    "            return\n",
    "        try:\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            loop = asyncio.get_event_loop()\n",
    "            loop.run_until_complete(self.client.ainsert(_SEED_DOCUMENT))\n",
    "            logger.info('GraphRetrieval: seed document inserted (fallback)')\n",
    "        except Exception as e:\n",
    "            logger.debug('GraphRetrieval: seed insert skipped: %s', e)\n",
    "        self._initialised = True\n",
    "\n",
    "    def find_top_k(self, query):\n",
    "        self._ensure_initialised()\n",
    "        try:\n",
    "            result = self.client.query(\n",
    "                query,\n",
    "                param=QueryParam(mode=self.mode, top_k=self.top_k)\n",
    "            )\n",
    "            return str(result) if result else ''\n",
    "        except Exception as e:\n",
    "            logger.error('GraphRetrieval error: %s', e)\n",
    "            return f'Graph retrieval failed: {e}'\n",
    "''')\n",
    "print(\"‚úì Patched retrieval/graph_retrieval.py\")\n",
    "\n",
    "# =============================================================================\n",
    "# PATCH 3: retrieval/web_retrieval.py ‚Äî Serper API\n",
    "# =============================================================================\n",
    "with open('retrieval/web_retrieval.py', 'w') as f:\n",
    "    f.write('''\"\"\"Web-based Retrieval Agent (HM-RAG Layer 2, Section 3.3.3).\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import requests\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "from retrieval.base_retrieval import BaseRetrieval\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "_SERPER_URL = \"https://google.serper.dev/search\"\n",
    "\n",
    "_SYNTHESIS_PROMPT = (\n",
    "    \"You are a helpful science question answering assistant.\\\\n\"\n",
    "    \"Below are search results retrieved from the web for the given question.\\\\n\"\n",
    "    \"Use ONLY the information in these search results to answer the question.\\\\n\"\n",
    "    \"If the results do not contain enough information, say so.\\\\n\"\n",
    "    \"Be concise and factual.\\\\n\\\\n\"\n",
    "    \"Search results:\\\\n{results}\\\\n\\\\n\"\n",
    "    \"Question: {query}\\\\n\\\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "\n",
    "class WebRetrieval(BaseRetrieval):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.serper_api_key = getattr(config, 'serper_api_key', '')\n",
    "        ollama_base_url = getattr(config, 'ollama_base_url', 'http://localhost:11434')\n",
    "        web_llm_model = getattr(config, 'web_llm_model_name', 'qwen2.5:1.5b')\n",
    "\n",
    "        self.llm = OllamaLLM(\n",
    "            base_url=ollama_base_url,\n",
    "            model=web_llm_model,\n",
    "            temperature=0.35,\n",
    "        )\n",
    "        logger.info('WebRetrieval initialised | top_k=%d | model=%s', self.top_k, web_llm_model)\n",
    "\n",
    "    def _serper_search(self, query):\n",
    "        if not self.serper_api_key:\n",
    "            raise RuntimeError('Serper API key is not set')\n",
    "        headers = {'X-API-KEY': self.serper_api_key, 'Content-Type': 'application/json'}\n",
    "        payload = {'q': query, 'num': self.top_k}\n",
    "        resp = requests.post(_SERPER_URL, json=payload, headers=headers, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "    def format_results(self, results):\n",
    "        if isinstance(results, str):\n",
    "            return results if results.strip() else 'No relevant results found.'\n",
    "        if not isinstance(results, dict):\n",
    "            return str(results) if results else 'No relevant results found.'\n",
    "        snippets = []\n",
    "        answer_box = results.get('answerBox')\n",
    "        if answer_box and isinstance(answer_box, dict):\n",
    "            answer_text = answer_box.get('answer') or answer_box.get('snippet') or ''\n",
    "            if answer_text:\n",
    "                snippets.append(f'Direct answer: {answer_text}')\n",
    "        for item in results.get('organic', [])[:self.top_k]:\n",
    "            title = item.get('title', 'No title')\n",
    "            snippet = item.get('snippet', 'No snippet')\n",
    "            snippets.append(f'[{title}]\\\\n{snippet}')\n",
    "        knowledge = results.get('knowledgeGraph')\n",
    "        if knowledge and isinstance(knowledge, dict):\n",
    "            desc = knowledge.get('description', '')\n",
    "            if desc:\n",
    "                snippets.append(f'Knowledge Graph: {desc}')\n",
    "        return '\\\\n\\\\n'.join(snippets) if snippets else 'No relevant results found.'\n",
    "\n",
    "    def _generate(self, search_results, query):\n",
    "        prompt = _SYNTHESIS_PROMPT.format(results=search_results, query=query)\n",
    "        try:\n",
    "            answer = self.llm.invoke(prompt)\n",
    "            return answer.strip() if answer else ''\n",
    "        except Exception as e:\n",
    "            logger.error('WebRetrieval generation failed: %s', e)\n",
    "            return f'Web generation failed: {e}'\n",
    "\n",
    "    def find_top_k(self, query):\n",
    "        try:\n",
    "            raw_results = self._serper_search(query)\n",
    "            formatted = self.format_results(raw_results)\n",
    "            answer = self._generate(formatted, query)\n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error('WebRetrieval failed: %s', e)\n",
    "            return f'Web retrieval failed: {e}'\n",
    "''')\n",
    "print(\"‚úì Patched retrieval/web_retrieval.py (Serper API)\")\n",
    "\n",
    "# =============================================================================\n",
    "# PATCH 4: agents/decompose_agent.py\n",
    "# =============================================================================\n",
    "with open('agents/decompose_agent.py', 'w') as f:\n",
    "    f.write('''\"\"\"Decomposition Agent (HM-RAG Layer 1).\"\"\"\n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "\n",
    "class DecomposeAgent:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.llm = OllamaLLM(\n",
    "            base_url=getattr(config, 'ollama_base_url', 'http://localhost:11434'),\n",
    "            model=getattr(config, 'llm_model_name', 'qwen2.5:1.5b'),\n",
    "            temperature=getattr(config, 'temperature', 0.35),\n",
    "        )\n",
    "\n",
    "    def count_intents(self, query: str) -> int:\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"Please calculate how many independent intents are contained in the following query. \"\n",
    "            \"Return only an integer:\\\\n{query}\\\\nNumber of intents: \"\n",
    "        )\n",
    "        for attempt in range(3):\n",
    "            response = self.llm.invoke(prompt.format(query=query))\n",
    "            numbers = re.findall(r'\\\\d+', response.strip())\n",
    "            if numbers:\n",
    "                return int(numbers[0])\n",
    "        return 1\n",
    "\n",
    "    def decompose(self, query: str) -> List[str]:\n",
    "        intent_count = min(self.count_intents(query), 3)\n",
    "        if intent_count > 1:\n",
    "            return self._split_query(query)\n",
    "        return [query]\n",
    "\n",
    "    def _split_query(self, query: str) -> List[str]:\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"Split the following query into multiple independent sub-queries, \"\n",
    "            \"separated by '||', without additional explanations:\\\\n{query}\\\\nList of sub-queries: \"\n",
    "        )\n",
    "        response = self.llm.invoke(prompt.format(query=query))\n",
    "        sub_queries = [q.strip() for q in response.split(\"||\") if q.strip()]\n",
    "        return sub_queries if sub_queries else [query]\n",
    "''')\n",
    "print(\"‚úì Patched agents/decompose_agent.py\")\n",
    "\n",
    "# =============================================================================\n",
    "# PATCH 5: agents/summary_agent.py ‚Äî vision model with load-once guard\n",
    "# =============================================================================\n",
    "with open('agents/summary_agent.py', 'w') as f:\n",
    "    f.write('''\"\"\"Decision Agent (HM-RAG Layer 3) ‚Äî voting + expert refinement.\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "from langchain_ollama import OllamaLLM\n",
    "import re\n",
    "from transformers import AutoProcessor\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from prompts.base_prompt import build_prompt\n",
    "\n",
    "\n",
    "class SummaryAgent:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.text_llm = OllamaLLM(\n",
    "            base_url=getattr(config, 'ollama_base_url', 'http://localhost:11434'),\n",
    "            model=getattr(config, 'llm_model_name', 'qwen2.5:1.5b')\n",
    "        )\n",
    "        self.hf_token = getattr(config, 'hf_token', '') or os.environ.get('HF_TOKEN', '')\n",
    "        self._vision_model = None\n",
    "        self._processor = None\n",
    "        self._vision_load_attempted = False\n",
    "\n",
    "    def _load_vision_model(self):\n",
    "        if self._vision_load_attempted:\n",
    "            return\n",
    "        self._vision_load_attempted = True\n",
    "        try:\n",
    "            from transformers import Qwen2VLForConditionalGeneration\n",
    "\n",
    "            model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "            token_kwargs = {}\n",
    "            if self.hf_token:\n",
    "                token_kwargs['token'] = self.hf_token\n",
    "\n",
    "            self._processor = AutoProcessor.from_pretrained(\n",
    "                model_name, use_fast=True, **token_kwargs\n",
    "            )\n",
    "            self._vision_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "                **token_kwargs\n",
    "            )\n",
    "            print(f\"‚úì Vision model {model_name} loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load vision model: {e}\")\n",
    "            self._vision_model = None\n",
    "            self._processor = None\n",
    "\n",
    "    def summarize(self, problems, shot_qids, qid, cur_ans):\n",
    "        problem = problems[qid]\n",
    "        question = problem['question']\n",
    "        choices = problem[\"choices\"]\n",
    "        answer = problem['answer']\n",
    "        image = problem.get('image', '')\n",
    "        split = problem.get(\"split\", \"test\")\n",
    "\n",
    "        most_ans = self.get_most_common_answer(cur_ans)\n",
    "\n",
    "        if len(most_ans) == 1:\n",
    "            prediction = self.get_result(most_ans[0])\n",
    "            pred_idx = self.get_pred_idx(prediction, choices, self.config.options)\n",
    "        else:\n",
    "            if image and image == \"image.png\":\n",
    "                image_path = os.path.join(self.config.image_root, split, qid, image)\n",
    "            else:\n",
    "                image_path = \"\"\n",
    "\n",
    "            output_text = cur_ans[0] if len(cur_ans) > 0 else \"\"\n",
    "            output_graph = cur_ans[1] if len(cur_ans) > 1 else \"\"\n",
    "            output_web = cur_ans[2] if len(cur_ans) > 2 else \"\"\n",
    "\n",
    "            output = self.refine(output_text, output_graph, output_web,\n",
    "                                 problems, shot_qids, qid, self.config, image_path)\n",
    "            if output is None:\n",
    "                output = \"FAILED\"\n",
    "            print(f\"output: {output}\")\n",
    "            ans_fusion = self.get_result(output)\n",
    "            pred_idx = self.get_pred_idx(ans_fusion, choices, self.config.options)\n",
    "        return pred_idx, cur_ans\n",
    "\n",
    "    def get_most_common_answer(self, res):\n",
    "        if not res:\n",
    "            return []\n",
    "        counter = Counter(res)\n",
    "        max_count = max(counter.values())\n",
    "        return [item for item, count in counter.items() if count == max_count]\n",
    "\n",
    "    def refine(self, output_text, output_graph, output_web, problems, shot_qids, qid, args, image_path):\n",
    "        prompt = build_prompt(problems, shot_qids, qid, args)\n",
    "        prompt = f\"{prompt} The answer is A, B, C, D, E or FAILED. \\\\n BECAUSE: \"\n",
    "\n",
    "        if not image_path:\n",
    "            output = self.text_llm.invoke(prompt)\n",
    "        else:\n",
    "            output = self.qwen_reasoning(prompt, image_path)\n",
    "            if output:\n",
    "                output = self.text_llm.invoke(\n",
    "                    f\"{output[0]} Summary the above information with format \"\n",
    "                    f\"'Answer: The answer is A, B, C, D, E or FAILED.\\\\n BECAUSE: '\"\n",
    "                )\n",
    "            else:\n",
    "                output = self.text_llm.invoke(prompt)\n",
    "        return output\n",
    "\n",
    "    def get_result(self, output):\n",
    "        pattern = re.compile(r'The answer is ([A-E])')\n",
    "        res = pattern.findall(output)\n",
    "        return res[0] if len(res) == 1 else \"FAILED\"\n",
    "\n",
    "    def get_pred_idx(self, prediction, choices, options):\n",
    "        if prediction in options[:len(choices)]:\n",
    "            return options.index(prediction)\n",
    "        return random.choice(range(len(choices)))\n",
    "\n",
    "    def qwen_reasoning(self, prompt, image_path):\n",
    "        self._load_vision_model()\n",
    "        if self._vision_model is None or self._processor is None:\n",
    "            print(\"Warning: Vision model not available, falling back to text-only.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            from qwen_vl_utils import process_vision_info\n",
    "        except ImportError:\n",
    "            print(\"Warning: qwen_vl_utils not installed, falling back to text-only.\")\n",
    "            return None\n",
    "\n",
    "        if os.path.isfile(image_path) and not image_path.startswith(('http://', 'https://', 'file://')):\n",
    "            image_uri = 'file://' + os.path.abspath(image_path)\n",
    "        else:\n",
    "            image_uri = image_path\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image_uri},\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        text = self._processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        inputs = self._processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        device = next(self._vision_model.parameters()).device\n",
    "        inputs = {k: v.to(device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
    "\n",
    "        generated_ids = self._vision_model.generate(**inputs, max_new_tokens=512)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs['input_ids'], generated_ids)\n",
    "        ]\n",
    "        output_text = self._processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        return output_text\n",
    "''')\n",
    "print(\"‚úì Patched agents/summary_agent.py (load-once guard, file:// prefix, Qwen2-VL-2B)\")\n",
    "\n",
    "# =============================================================================\n",
    "# PATCH 6: main.py ‚Äî add preprocessing step\n",
    "# =============================================================================\n",
    "with open('main.py', 'r') as f:\n",
    "    main_content = f.read()\n",
    "\n",
    "changes_made = []\n",
    "\n",
    "# Fix model name\n",
    "if \"default='qwen2.5:7b'\" in main_content:\n",
    "    main_content = main_content.replace(\"default='qwen2.5:7b'\", \"default='qwen2.5:1.5b'\")\n",
    "    changes_made.append(\"qwen2.5:7b -> qwen2.5:1.5b\")\n",
    "\n",
    "# Fix API key naming\n",
    "if '--serpapi_api_key' in main_content:\n",
    "    main_content = main_content.replace('--serpapi_api_key', '--serper_api_key')\n",
    "    main_content = main_content.replace('serpapi_api_key', 'serper_api_key')\n",
    "    changes_made.append(\"serpapi -> serper\")\n",
    "\n",
    "# Add --hf_token if missing\n",
    "if '--hf_token' not in main_content:\n",
    "    main_content = main_content.replace(\n",
    "        \"return parser.parse_args()\",\n",
    "        \"    parser.add_argument('--hf_token', type=str, default='',\\n\"\n",
    "        \"                        help='HF access token for gated models')\\n\"\n",
    "        \"    return parser.parse_args()\"\n",
    "    )\n",
    "    changes_made.append(\"Added --hf_token\")\n",
    "\n",
    "# Add preprocessing import and call\n",
    "if 'KnowledgeBaseBuilder' not in main_content:\n",
    "    # Add import\n",
    "    main_content = main_content.replace(\n",
    "        'from agents.multi_retrieval_agents import MRetrievalAgent',\n",
    "        'from agents.multi_retrieval_agents import MRetrievalAgent\\n'\n",
    "        'from preprocessing.build_knowledge_base import KnowledgeBaseBuilder'\n",
    "    )\n",
    "    # Add preprocessing call before agent init\n",
    "    main_content = main_content.replace(\n",
    "        '    agent = MRetrievalAgent(args)',\n",
    "        '    # Phase 1: Build Knowledge Base (Section 3.1)\\n'\n",
    "        '    splits_path = os.path.join(args.data_root, \"pid_splits.json\")\\n'\n",
    "        '    with open(splits_path, \"r\") as f:\\n'\n",
    "        '        _pid_splits = json.load(f)\\n'\n",
    "        '    train_qids = _pid_splits.get(\"train\", [])\\n'\n",
    "        '    builder = KnowledgeBaseBuilder(args)\\n'\n",
    "        '    builder.build(problems, train_qids)\\n'\n",
    "        '\\n'\n",
    "        '    agent = MRetrievalAgent(args)'\n",
    "    )\n",
    "    changes_made.append(\"Added preprocessing step\")\n",
    "\n",
    "# Add HF login if missing\n",
    "if '_setup_hf_token' not in main_content and 'HF_TOKEN' not in main_content:\n",
    "    main_content = main_content.replace(\n",
    "        '    agent = MRetrievalAgent(args)',\n",
    "        '    # Set HF token if provided\\n'\n",
    "        '    if hasattr(args, \"hf_token\") and args.hf_token:\\n'\n",
    "        '        os.environ[\"HF_TOKEN\"] = args.hf_token\\n'\n",
    "        '        os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = args.hf_token\\n'\n",
    "        '        try:\\n'\n",
    "        '            from huggingface_hub import login\\n'\n",
    "        '            login(token=args.hf_token)\\n'\n",
    "        '            print(\"‚úì Logged in to Hugging Face Hub\")\\n'\n",
    "        '        except Exception as e:\\n'\n",
    "        '            print(f\"Warning: Could not login to HF Hub: {e}\")\\n'\n",
    "        '\\n'\n",
    "        '    agent = MRetrievalAgent(args)'\n",
    "    )\n",
    "    changes_made.append(\"Added HF login\")\n",
    "\n",
    "with open('main.py', 'w') as f:\n",
    "    f.write(main_content)\n",
    "\n",
    "if changes_made:\n",
    "    print(\"‚úì Patched main.py: \" + \", \".join(changes_made))\n",
    "else:\n",
    "    print(\"‚úì main.py already up to date\")\n",
    "\n",
    "# =============================================================================\n",
    "# PATCH 7: YAML configs\n",
    "# =============================================================================\n",
    "for yaml_file in ['configs/decompose_agent.yaml', 'configs/multi_retrieval_agents.yaml']:\n",
    "    if os.path.exists(yaml_file):\n",
    "        with open(yaml_file, 'r') as f:\n",
    "            content = f.read()\n",
    "        updated = False\n",
    "        if 'qwen2.5:7b' in content:\n",
    "            content = content.replace('qwen2.5:7b', 'qwen2.5:1.5b')\n",
    "            updated = True\n",
    "        if 'serpapi_api_key' in content:\n",
    "            content = content.replace('serpapi_api_key', 'serper_api_key')\n",
    "            updated = True\n",
    "        if updated:\n",
    "            with open(yaml_file, 'w') as f:\n",
    "                f.write(content)\n",
    "            print(f\"‚úì Patched {yaml_file}\")\n",
    "        else:\n",
    "            print(f\"‚úì {yaml_file} already correct\")\n",
    "\n",
    "# =============================================================================\n",
    "# PATCH 8: Create preprocessing module (Phase 1, Section 3.1)\n",
    "# =============================================================================\n",
    "os.makedirs('preprocessing', exist_ok=True)\n",
    "\n",
    "with open('preprocessing/__init__.py', 'w') as f:\n",
    "    f.write('''\"\"\"Preprocessing module for HM-RAG (Phase 1, Section 3.1).\"\"\"\n",
    "from preprocessing.build_knowledge_base import KnowledgeBaseBuilder\n",
    "__all__ = [\"KnowledgeBaseBuilder\"]\n",
    "''')\n",
    "\n",
    "with open('preprocessing/build_knowledge_base.py', 'w') as f:\n",
    "    f.write('''\"\"\"\n",
    "Knowledge-Base Builder - HM-RAG Phase 1 (Section 3.1).\n",
    "\n",
    "Eq 1: D_img  = VLM(image)              - image-to-text via Qwen VLM\n",
    "Eq 2: D_comb = concat(D_text, D_img)   - merge textual + visual info\n",
    "Eq 3: KB     = LightRAG.insert(D_comb) - index into vector + graph DB\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import gc\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "_MARKER = \".kb_built\"\n",
    "_INSERT_BATCH = 50\n",
    "_VLM_MAX_TOKENS = 256\n",
    "\n",
    "\n",
    "class ImageCaptioner:\n",
    "    \"\"\"Generate captions using Qwen2-VL-2B-Instruct (~1.5B non-embedding params).\"\"\"\n",
    "\n",
    "    MODEL_ID = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "    def __init__(self, hf_token=\"\", device=None):\n",
    "        self.hf_token = hf_token\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._model = None\n",
    "        self._processor = None\n",
    "\n",
    "    def _load(self):\n",
    "        if self._model is not None:\n",
    "            return\n",
    "        from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "        token_kw = {\"token\": self.hf_token} if self.hf_token else {}\n",
    "        logger.info(\"Loading VLM %s ...\", self.MODEL_ID)\n",
    "        self._processor = AutoProcessor.from_pretrained(self.MODEL_ID, use_fast=True, **token_kw)\n",
    "        dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "        self._model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            self.MODEL_ID, torch_dtype=dtype,\n",
    "            device_map=\"auto\" if self.device == \"cuda\" else None,\n",
    "            **token_kw,\n",
    "        )\n",
    "        if self.device != \"cuda\":\n",
    "            self._model.to(self.device)\n",
    "        logger.info(\"VLM loaded on %s\", self.device)\n",
    "\n",
    "    def unload(self):\n",
    "        del self._model, self._processor\n",
    "        self._model = None\n",
    "        self._processor = None\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        logger.info(\"VLM unloaded\")\n",
    "\n",
    "    def caption(self, image_path):\n",
    "        if not image_path or not os.path.isfile(image_path):\n",
    "            return \"\"\n",
    "        self._load()\n",
    "        try:\n",
    "            from qwen_vl_utils import process_vision_info\n",
    "            messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": f\"file://{image_path}\"},\n",
    "                    {\"type\": \"text\", \"text\": (\n",
    "                        \"Describe this image in detail for a science \"\n",
    "                        \"question-answering system. Include all visible \"\n",
    "                        \"text, labels, diagrams, charts, and relevant \"\n",
    "                        \"scientific information.\"\n",
    "                    )},\n",
    "                ],\n",
    "            }]\n",
    "            text = self._processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "            inputs = self._processor(text=[text], images=image_inputs, videos=video_inputs,\n",
    "                                     padding=True, return_tensors=\"pt\")\n",
    "            device = next(self._model.parameters()).device\n",
    "            inputs = {k: v.to(device) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n",
    "            with torch.inference_mode():\n",
    "                gen_ids = self._model.generate(**inputs, max_new_tokens=_VLM_MAX_TOKENS)\n",
    "            trimmed = [out[len(inp):] for inp, out in zip(inputs[\"input_ids\"], gen_ids)]\n",
    "            return self._processor.batch_decode(trimmed, skip_special_tokens=True)[0].strip()\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Caption failed for %s: %s\", image_path, e)\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "def _build_document(problem, qid, image_caption):\n",
    "    parts = []\n",
    "    subject = problem.get(\"subject\", \"\")\n",
    "    topic = problem.get(\"topic\", \"\")\n",
    "    if subject or topic:\n",
    "        parts.append(f\"Subject: {subject}  Topic: {topic}\")\n",
    "\n",
    "    q = problem.get(\"question\", \"\")\n",
    "    choices = problem.get(\"choices\", [])\n",
    "    if q:\n",
    "        opts = \" | \".join(f\"({chr(65+i)}) {c}\" for i, c in enumerate(choices))\n",
    "        parts.append(f\"Question: {q}\\\\nOptions: {opts}\")\n",
    "\n",
    "    hint = (problem.get(\"hint\") or \"\").strip()\n",
    "    if hint:\n",
    "        parts.append(f\"Hint: {hint}\")\n",
    "\n",
    "    lecture = (problem.get(\"lecture\") or \"\").strip()\n",
    "    if lecture:\n",
    "        parts.append(f\"Lecture: {lecture}\")\n",
    "\n",
    "    solution = (problem.get(\"solution\") or \"\").strip()\n",
    "    if solution:\n",
    "        parts.append(f\"Solution: {solution}\")\n",
    "\n",
    "    ds_caption = (problem.get(\"caption\") or \"\").strip()\n",
    "    if ds_caption:\n",
    "        parts.append(f\"Image caption: {ds_caption}\")\n",
    "\n",
    "    if image_caption:\n",
    "        parts.append(f\"Image description (VLM): {image_caption}\")\n",
    "\n",
    "    answer_idx = problem.get(\"answer\")\n",
    "    if answer_idx is not None and answer_idx < len(choices):\n",
    "        parts.append(f\"Correct answer: ({chr(65+answer_idx)}) {choices[answer_idx]}\")\n",
    "\n",
    "    return \"\\\\n\".join(parts)\n",
    "\n",
    "\n",
    "class KnowledgeBaseBuilder:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.working_dir = getattr(config, \"working_dir\", \"./lightrag_workdir\")\n",
    "        self.image_root = getattr(config, \"image_root\", \"\")\n",
    "        self.hf_token = getattr(config, \"hf_token\", \"\") or os.environ.get(\"HF_TOKEN\", \"\")\n",
    "\n",
    "    def build(self, problems, qids):\n",
    "        marker_path = os.path.join(self.working_dir, _MARKER)\n",
    "        if os.path.exists(marker_path):\n",
    "            logger.info(\"Knowledge base already built ‚Äî skipping. Delete %s to rebuild.\", marker_path)\n",
    "            return\n",
    "\n",
    "        os.makedirs(self.working_dir, exist_ok=True)\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"Phase 1: Building Knowledge Base (%d problems)\", len(qids))\n",
    "        logger.info(\"=\" * 60)\n",
    "\n",
    "        captions = self._caption_images(problems, qids)\n",
    "        documents = self._assemble_documents(problems, qids, captions)\n",
    "        self._index_documents(documents)\n",
    "\n",
    "        with open(marker_path, \"w\") as f:\n",
    "            f.write(f\"Built from {len(documents)} documents\\\\n\")\n",
    "        logger.info(\"Phase 1 complete ‚Äî marker written to %s\", marker_path)\n",
    "\n",
    "    def _caption_images(self, problems, qids):\n",
    "        cache_path = os.path.join(self.working_dir, \"vlm_captions.json\")\n",
    "        if os.path.exists(cache_path):\n",
    "            logger.info(\"Loading cached VLM captions from %s\", cache_path)\n",
    "            with open(cache_path, \"r\") as f:\n",
    "                return json.load(f)\n",
    "\n",
    "        image_qids = []\n",
    "        for qid in qids:\n",
    "            img = problems[qid].get(\"image\", \"\")\n",
    "            if img:\n",
    "                split = problems[qid].get(\"split\", \"train\")\n",
    "                img_path = os.path.join(self.image_root, split, qid, img)\n",
    "                if os.path.isfile(img_path):\n",
    "                    image_qids.append((qid, img_path))\n",
    "\n",
    "        captions = {qid: \"\" for qid in qids}\n",
    "        if not image_qids:\n",
    "            logger.info(\"No images found ‚Äî skipping VLM captioning\")\n",
    "            return captions\n",
    "\n",
    "        logger.info(\"Captioning %d images with VLM ...\", len(image_qids))\n",
    "        captioner = ImageCaptioner(hf_token=self.hf_token)\n",
    "        try:\n",
    "            for i, (qid, img_path) in enumerate(image_qids):\n",
    "                captions[qid] = captioner.caption(img_path)\n",
    "                if (i + 1) % 50 == 0 or (i + 1) == len(image_qids):\n",
    "                    logger.info(\"  captioned %d / %d images\", i + 1, len(image_qids))\n",
    "        finally:\n",
    "            captioner.unload()\n",
    "\n",
    "        with open(cache_path, \"w\") as f:\n",
    "            json.dump(captions, f)\n",
    "        logger.info(\"VLM captions cached to %s\", cache_path)\n",
    "        return captions\n",
    "\n",
    "    def _assemble_documents(self, problems, qids, captions):\n",
    "        documents = []\n",
    "        for qid in qids:\n",
    "            doc = _build_document(problems[qid], qid, captions.get(qid, \"\"))\n",
    "            if doc.strip():\n",
    "                documents.append(doc)\n",
    "        logger.info(\"Assembled %d documents\", len(documents))\n",
    "        return documents\n",
    "\n",
    "    def _index_documents(self, documents):\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "\n",
    "        from lightrag import LightRAG\n",
    "        from lightrag.llm.ollama import ollama_model_complete, ollama_embed\n",
    "        from lightrag.utils import EmbeddingFunc\n",
    "\n",
    "        ollama_host = getattr(self.config, \"ollama_base_url\", \"http://localhost:11434\")\n",
    "        model_name = getattr(self.config, \"llm_model_name\", \"qwen2.5:1.5b\")\n",
    "\n",
    "        rag = LightRAG(\n",
    "            working_dir=self.working_dir,\n",
    "            llm_model_func=ollama_model_complete,\n",
    "            llm_model_name=model_name,\n",
    "            llm_model_max_async=4,\n",
    "            llm_model_kwargs={\"host\": ollama_host, \"options\": {\"num_ctx\": 4096}},\n",
    "            embedding_func=EmbeddingFunc(\n",
    "                embedding_dim=768,\n",
    "                max_token_size=8192,\n",
    "                func=lambda texts: ollama_embed.func(\n",
    "                    texts, embed_model=\"nomic-embed-text\", host=ollama_host,\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        total = len(documents)\n",
    "        logger.info(\"Indexing %d documents into LightRAG ...\", total)\n",
    "        loop = asyncio.get_event_loop()\n",
    "        for start in range(0, total, _INSERT_BATCH):\n",
    "            batch = documents[start : start + _INSERT_BATCH]\n",
    "            combined = \"\\\\n\\\\n---\\\\n\\\\n\".join(batch)\n",
    "            try:\n",
    "                loop.run_until_complete(rag.ainsert(combined))\n",
    "            except Exception as e:\n",
    "                logger.error(\"Insert failed for batch %d-%d: %s\", start, start + len(batch), e)\n",
    "            done = min(start + _INSERT_BATCH, total)\n",
    "            logger.info(\"  indexed %d / %d documents\", done, total)\n",
    "        logger.info(\"LightRAG indexing complete\")\n",
    "''')\n",
    "print(\"‚úì Created preprocessing/build_knowledge_base.py (Phase 1: VLM captioning + LightRAG indexing)\")\n",
    "\n",
    "# =============================================================================\n",
    "# PATCH 9: Install nest_asyncio + Pillow\n",
    "# =============================================================================\n",
    "!pip install -q nest_asyncio Pillow\n",
    "\n",
    "# Clean up stale working directory (preprocessing will rebuild it)\n",
    "!rm -rf ./lightrag_workdir\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì All patches applied!\")\n",
    "print(\"  - embedding_dim = 768 (matches nomic-embed-text)\")\n",
    "print(\"  - ollama_embed.func (bypasses 1024-dim decorator)\")\n",
    "print(\"  - num_ctx = 4096 (fits qwen2.5:1.5b)\")\n",
    "print(\"  - Text model: qwen2.5:1.5b (via Ollama)\")\n",
    "print(\"  - LightRAG LLM: ollama_model_complete (NOT GPT-4o-mini)\")\n",
    "print(\"  - Vision model: Qwen/Qwen2-VL-2B-Instruct\")\n",
    "print(\"  - Web search: Serper API (https://serper.dev)\")\n",
    "print(\"  - HF token support added\")\n",
    "print(\"  - Phase 1 preprocessing: VLM captioning + LightRAG indexing\")\n",
    "print(\"  - Knowledge base is built from training data before evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be45fc17",
   "metadata": {},
   "source": [
    "## Step 4: Install and Start Ollama + Pull Models\n",
    "Ollama runs locally on the Colab VM. We pull `qwen2.5:1.5b` (~1GB) and `nomic-embed-text` (~270MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Install system dependencies\n",
    "print(\"Installing system dependencies...\")\n",
    "!sudo apt-get update -qq 2>/dev/null\n",
    "!sudo apt-get install -y -qq zstd 2>/dev/null\n",
    "\n",
    "# Install Ollama\n",
    "print(\"Installing Ollama...\")\n",
    "!curl -fsSL https://ollama.com/install.sh | sh 2>&1 | tail -3\n",
    "\n",
    "# Find ollama binary\n",
    "result = subprocess.run(['which', 'ollama'], capture_output=True, text=True)\n",
    "ollama_path = result.stdout.strip()\n",
    "if not ollama_path:\n",
    "    for path in ['/usr/local/bin/ollama', '/usr/bin/ollama']:\n",
    "        if os.path.exists(path):\n",
    "            ollama_path = path\n",
    "            break\n",
    "\n",
    "if not ollama_path:\n",
    "    print(\"‚ùå Ollama binary not found! Please restart runtime and try again.\")\n",
    "else:\n",
    "    print(f\"‚úì Ollama found at: {ollama_path}\")\n",
    "    \n",
    "    # Kill any existing ollama processes\n",
    "    subprocess.run(['pkill', '-f', 'ollama'], stderr=subprocess.DEVNULL)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Start Ollama server in background\n",
    "    print(\"Starting Ollama server...\")\n",
    "    ollama_process = subprocess.Popen(\n",
    "        [ollama_path, 'serve'],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    time.sleep(8)\n",
    "    \n",
    "    # Verify server is running\n",
    "    result = subprocess.run(['curl', '-s', 'http://localhost:11434/api/tags'],\n",
    "                          capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úì Ollama server is running!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Ollama server may not be ready yet. Waiting more...\")\n",
    "        time.sleep(10)\n",
    "    \n",
    "    # Pull the text model\n",
    "    print(\"\\nPulling qwen2.5:1.5b model (~1GB, may take 2-5 min)...\")\n",
    "    !{ollama_path} pull qwen2.5:1.5b\n",
    "    \n",
    "    # Pull the embedding model\n",
    "    print(\"\\nPulling nomic-embed-text model (~270MB)...\")\n",
    "    !{ollama_path} pull nomic-embed-text\n",
    "    \n",
    "    print(\"\\n‚úì Ollama setup complete!\")\n",
    "    print(\"\\nAvailable models:\")\n",
    "    !{ollama_path} list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0371b5",
   "metadata": {},
   "source": [
    "## Step 5: Configure API Keys\n",
    "- **Serper API key** (required for web search): Get a free key at https://serper.dev (2,500 free searches)\n",
    "- **HF token** (optional, for gated models): Get from https://huggingface.co/settings/tokens\n",
    "\n",
    "You can set them via Colab's **Secrets** (left sidebar ‚Üí üîë icon) or paste directly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea63e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# =====================================================\n",
    "# OPTION 1: Use Colab Secrets (recommended)\n",
    "# Add SERPER_API_KEY and HF_TOKEN in left sidebar ‚Üí üîë\n",
    "# =====================================================\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    SERPER_API_KEY = userdata.get('SERPER_API_KEY')\n",
    "    print(\"‚úì SERPER_API_KEY loaded from Colab Secrets\")\n",
    "except Exception:\n",
    "    # OPTION 2: Paste your key directly here\n",
    "    SERPER_API_KEY = \"\"  # <-- PASTE YOUR SERPER KEY HERE (from https://serper.dev)\n",
    "    if SERPER_API_KEY:\n",
    "        print(\"‚úì SERPER_API_KEY set manually\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è SERPER_API_KEY not set! Web search will fail.\")\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"‚úì HF_TOKEN loaded from Colab Secrets\")\n",
    "except Exception:\n",
    "    # OPTION 2: Paste your HF token directly here\n",
    "    HF_TOKEN = \"\"  # <-- PASTE YOUR HF TOKEN HERE (optional)\n",
    "    if HF_TOKEN:\n",
    "        print(\"‚úì HF_TOKEN set manually\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è HF_TOKEN not set (optional ‚Äî only needed for gated models)\")\n",
    "\n",
    "# Store in environment for the subprocess calls\n",
    "os.environ['SERPER_API_KEY'] = SERPER_API_KEY or ''\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN or ''\n",
    "\n",
    "print(\"\\nAPI keys configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ca8a8d",
   "metadata": {},
   "source": [
    "## Step 6: Download ScienceQA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f932419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/HMRAG')\n",
    "\n",
    "# Create dataset directory\n",
    "os.makedirs('dataset', exist_ok=True)\n",
    "os.chdir('dataset')\n",
    "\n",
    "# Clone the ScienceQA repository\n",
    "if not os.path.exists('ScienceQA'):\n",
    "    print(\"Cloning ScienceQA repository...\")\n",
    "    !git clone https://github.com/lupantech/ScienceQA\n",
    "else:\n",
    "    print(\"‚úì ScienceQA directory already exists\")\n",
    "\n",
    "if os.path.exists('ScienceQA'):\n",
    "    os.chdir('ScienceQA')\n",
    "    \n",
    "    # Download the dataset\n",
    "    if os.path.exists('tools/download.sh'):\n",
    "        print(\"Downloading dataset files (this may take several minutes)...\")\n",
    "        !bash tools/download.sh\n",
    "    else:\n",
    "        print(\"download.sh not found, creating data directory...\")\n",
    "        os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    os.chdir('/content/HMRAG')\n",
    "\n",
    "# Verify dataset structure\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Checking required files:\")\n",
    "required_files = [\n",
    "    'dataset/ScienceQA/data/scienceqa/problems.json',\n",
    "    'dataset/ScienceQA/data/scienceqa/pid_splits.json'\n",
    "]\n",
    "\n",
    "# Also check alternative locations\n",
    "alt_files = [\n",
    "    'dataset/ScienceQA/data/problems.json',\n",
    "    'dataset/ScienceQA/data/pid_splits.json'\n",
    "]\n",
    "\n",
    "data_root = None\n",
    "for f in required_files:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"‚úì Found: {f}\")\n",
    "        if 'problems.json' in f:\n",
    "            data_root = os.path.dirname(f)\n",
    "    else:\n",
    "        print(f\"  Not at: {f}\")\n",
    "\n",
    "if data_root is None:\n",
    "    for f in alt_files:\n",
    "        if os.path.exists(f):\n",
    "            print(f\"‚úì Found: {f}\")\n",
    "            if 'problems.json' in f:\n",
    "                data_root = os.path.dirname(f)\n",
    "        else:\n",
    "            print(f\"  Not at: {f}\")\n",
    "\n",
    "if data_root:\n",
    "    print(f\"\\n‚úì Data root: {data_root}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Could not find problems.json automatically.\")\n",
    "    print(\"Please check the dataset structure manually:\")\n",
    "    !find dataset/ScienceQA -name \"problems.json\" 2>/dev/null | head -5\n",
    "    print(\"\\nYou'll need to set --data_root accordingly in the run command.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e0202",
   "metadata": {},
   "source": [
    "## Step 7: Verify Everything Before Running\n",
    "Quick check that Ollama server is running and models are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "os.chdir('/content/HMRAG')\n",
    "\n",
    "# Check Ollama server\n",
    "print(\"=\" * 50)\n",
    "print(\"CHECKING OLLAMA SERVER\")\n",
    "print(\"=\" * 50)\n",
    "try:\n",
    "    result = subprocess.run(['curl', '-s', 'http://localhost:11434/api/tags'],\n",
    "                          capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úì Ollama server is running!\")\n",
    "    else:\n",
    "        raise Exception(\"Not responding\")\n",
    "except Exception:\n",
    "    print(\"‚ö†Ô∏è Ollama server not running. Restarting...\")\n",
    "    subprocess.run(['pkill', '-f', 'ollama'], stderr=subprocess.DEVNULL)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    result = subprocess.run(['which', 'ollama'], capture_output=True, text=True)\n",
    "    ollama_path = result.stdout.strip() or '/usr/local/bin/ollama'\n",
    "    \n",
    "    subprocess.Popen([ollama_path, 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    time.sleep(8)\n",
    "    \n",
    "    result = subprocess.run(['curl', '-s', 'http://localhost:11434/api/tags'],\n",
    "                          capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úì Ollama server restarted!\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to start Ollama. Restart runtime and rerun.\")\n",
    "\n",
    "print(\"\\nAvailable models:\")\n",
    "!ollama list\n",
    "\n",
    "# Check critical files\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CHECKING SOURCE FILES\")\n",
    "print(\"=\" * 50)\n",
    "critical_files = [\n",
    "    'main.py',\n",
    "    'agents/decompose_agent.py',\n",
    "    'agents/summary_agent.py',\n",
    "    'agents/multi_retrieval_agents.py',\n",
    "    'retrieval/vector_retrieval.py',\n",
    "    'retrieval/graph_retrieval.py',\n",
    "    'retrieval/web_retrieval.py',\n",
    "    'retrieval/base_retrieval.py',\n",
    "    'prompts/base_prompt.py',\n",
    "]\n",
    "for f in critical_files:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"‚úì {f}\")\n",
    "    else:\n",
    "        print(f\"‚ùå MISSING: {f}\")\n",
    "\n",
    "# Quick import test\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TESTING IMPORTS\")\n",
    "print(\"=\" * 50)\n",
    "try:\n",
    "    import sys\n",
    "    sys.path.insert(0, '/content/HMRAG')\n",
    "    import requests\n",
    "    print(\"‚úì requests (for Serper API)\")\n",
    "    from langchain_ollama import OllamaLLM\n",
    "    print(\"‚úì OllamaLLM (langchain-ollama)\")\n",
    "    from lightrag import LightRAG\n",
    "    print(\"‚úì LightRAG (lightrag-hku)\")\n",
    "    from lightrag.llm.ollama import ollama_model_complete, ollama_embed\n",
    "    print(\"‚úì ollama_model_complete, ollama_embed (Ollama backend for LightRAG)\")\n",
    "    print(\"\\n‚úì All imports successful!\")\n",
    "    print(\"  LightRAG LLM: Ollama qwen2.5:1.5b (NOT GPT-4o-mini)\")\n",
    "    print(\"  Web search: Serper API (requests.post)\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Try rerunning Step 2 (Install Dependencies)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa9373",
   "metadata": {},
   "source": [
    "## Step 8: Run Inference ‚Äî Small Test (5 examples)\n",
    "**Phase 1 (Preprocessing)** runs automatically on first launch and builds the knowledge base from training data. This is cached in `./lightrag_workdir/` ‚Äî subsequent runs skip this step.\n",
    "\n",
    "**To force a rebuild**, delete `./lightrag_workdir/.kb_built` (or the entire directory).\n",
    "\n",
    "Adjust `--data_root` below if your dataset location differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f502958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/HMRAG')\n",
    "\n",
    "# The preprocessing step (Phase 1) will build the knowledge base on\n",
    "# first run and cache it in ./lightrag_workdir/.  To force a rebuild:\n",
    "#   !rm -rf ./lightrag_workdir\n",
    "!mkdir -p outputs\n",
    "\n",
    "# Auto-detect data_root\n",
    "data_root = \"\"\n",
    "for candidate in [\n",
    "    \"./dataset/ScienceQA/data/scienceqa\",\n",
    "    \"./dataset/ScienceQA/data\",\n",
    "]:\n",
    "    if os.path.exists(os.path.join(candidate, \"problems.json\")):\n",
    "        data_root = candidate\n",
    "        break\n",
    "\n",
    "if not data_root:\n",
    "    print(\"‚ùå Could not find problems.json. Please set data_root manually.\")\n",
    "    print(\"Searching for it...\")\n",
    "    !find dataset/ -name \"problems.json\" 2>/dev/null\n",
    "else:\n",
    "    print(f\"Using data_root: {data_root}\")\n",
    "    \n",
    "    # Build the command\n",
    "    serper_key = os.environ.get('SERPER_API_KEY', '')\n",
    "    hf_token = os.environ.get('HF_TOKEN', '')\n",
    "    \n",
    "    cmd = f\"\"\"python3 main.py \\\n",
    "    --data_root {data_root} \\\n",
    "    --image_root ./dataset/ScienceQA/data/scienceqa \\\n",
    "    --output_root ./outputs \\\n",
    "    --working_dir ./lightrag_workdir \\\n",
    "    --serper_api_key \"{serper_key}\" \\\n",
    "    --llm_model_name qwen2.5:1.5b \\\n",
    "    --web_llm_model_name qwen2.5:1.5b \\\n",
    "    --test_split test \\\n",
    "    --test_number 5 \\\n",
    "    --shot_number 0 \\\n",
    "    --label test_run \\\n",
    "    --save_every 5\"\"\"\n",
    "    \n",
    "    if hf_token:\n",
    "        cmd += f' --hf_token \"{hf_token}\"'\n",
    "    \n",
    "    print(f\"\\nRunning command:\\n{cmd}\\n\")\n",
    "    !{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86214ea",
   "metadata": {},
   "source": [
    "## Step 9: Run Full Inference\n",
    "After the small test works, run on the full test set. This will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef117fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/HMRAG')\n",
    "\n",
    "# Knowledge base in ./lightrag_workdir is reused from previous runs.\n",
    "# To force a full rebuild: !rm -rf ./lightrag_workdir\n",
    "\n",
    "# Auto-detect data_root\n",
    "data_root = \"\"\n",
    "for candidate in [\n",
    "    \"./dataset/ScienceQA/data/scienceqa\",\n",
    "    \"./dataset/ScienceQA/data\",\n",
    "]:\n",
    "    if os.path.exists(os.path.join(candidate, \"problems.json\")):\n",
    "        data_root = candidate\n",
    "        break\n",
    "\n",
    "serper_key = os.environ.get('SERPER_API_KEY', '')\n",
    "hf_token = os.environ.get('HF_TOKEN', '')\n",
    "\n",
    "cmd = f\"\"\"python3 main.py \\\n",
    "    --data_root {data_root} \\\n",
    "    --image_root ./dataset/ScienceQA/data/scienceqa \\\n",
    "    --output_root ./outputs \\\n",
    "    --working_dir ./lightrag_workdir \\\n",
    "    --serper_api_key \"{serper_key}\" \\\n",
    "    --llm_model_name qwen2.5:1.5b \\\n",
    "    --web_llm_model_name qwen2.5:1.5b \\\n",
    "    --test_split test \\\n",
    "    --shot_number 2 \\\n",
    "    --label full_run \\\n",
    "    --save_every 50 \\\n",
    "    --use_caption\"\"\"\n",
    "\n",
    "if hf_token:\n",
    "    cmd += f' --hf_token \"{hf_token}\"'\n",
    "\n",
    "print(f\"Running command:\\n{cmd}\\n\")\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94fceda",
   "metadata": {},
   "source": [
    "## Step 10: View & Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73943b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "os.chdir('/content/HMRAG')\n",
    "\n",
    "# List output files\n",
    "print(\"Output files:\")\n",
    "!ls -lh outputs/\n",
    "\n",
    "# Load and display results\n",
    "output_files = sorted(glob.glob('outputs/*.json'))\n",
    "if output_files:\n",
    "    for fpath in output_files:\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(f\"File: {os.path.basename(fpath)}\")\n",
    "        with open(fpath, 'r') as f:\n",
    "            results = json.load(f)\n",
    "        print(f\"Total results: {len(results)}\")\n",
    "        print(\"Sample results:\")\n",
    "        for qid, answer in list(results.items())[:5]:\n",
    "            print(f\"  Question {qid}: Answer = {answer}\")\n",
    "else:\n",
    "    print(\"No output files found yet. Run inference first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91843237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results to your local machine\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "os.chdir('/content/HMRAG')\n",
    "\n",
    "# Zip all outputs\n",
    "!zip -r outputs.zip outputs/\n",
    "files.download('outputs.zip')\n",
    "print(\"‚úì Download started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f5088",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### \"Dimension mismatch\" / \"embedding_dim\" errors\n",
    "```\n",
    "rm -rf ./lightrag_workdir\n",
    "```\n",
    "Then rerun. This happens when a previous run created the DB with a different embedding dimension.\n",
    "\n",
    "### Preprocessing takes too long\n",
    "The first run builds the knowledge base from training data (Phase 1). This is cached ‚Äî subsequent runs skip it. To force a rebuild:\n",
    "```\n",
    "rm ./lightrag_workdir/.kb_built\n",
    "```\n",
    "\n",
    "### VLM captioning is slow\n",
    "Image captioning uses `Qwen2-VL-2B-Instruct`. On Colab T4 GPU, expect ~1-2 seconds per image. Captions are cached in `./lightrag_workdir/vlm_captions.json`.\n",
    "\n",
    "### \"Ollama connection refused\"\n",
    "Re-run the Ollama setup cell (Step 4) or the verification cell (Step 7).\n",
    "\n",
    "### \"Serper API error\"\n",
    "Make sure your `SERPER_API_KEY` is set correctly in Step 5. Get a free key at https://serper.dev (2,500 free searches).\n",
    "\n",
    "### \"CUDA out of memory\"\n",
    "The notebook uses `qwen2.5:1.5b` (text) and `Qwen/Qwen2-VL-2B-Instruct` (vision). If you still run out of memory:\n",
    "- Use `Runtime ‚Üí Change runtime type ‚Üí T4 GPU`\n",
    "- Restart runtime and rerun all cells\n",
    "- The VLM is automatically unloaded after preprocessing to free GPU memory\n",
    "\n",
    "### LightRAG LLM\n",
    "LightRAG is configured to use `ollama_model_complete` with `qwen2.5:1.5b` ‚Äî it does **NOT** use GPT-4o-mini. No OpenAI API key is needed."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
