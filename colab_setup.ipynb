{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba9d365",
   "metadata": {},
   "source": [
    "# HM-RAG: Hierarchical Multi-Agent Multimodal RAG ‚Äî Google Colab Setup\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Clones the repo\n",
    "2. Installs all dependencies (from `requirements.txt`)\n",
    "3. Installs & starts Ollama, pulls `qwen2.5:1.5b` + `nomic-embed-text`\n",
    "4. Patches source files to fix the **embedding dimension mismatch** (768 vs 1024)\n",
    "5. Downloads ScienceQA dataset\n",
    "6. Configures API keys (SerpAPI for web search, optional HF token)\n",
    "7. Runs inference\n",
    "\n",
    "**Model used:** `qwen2.5:1.5b` (text) + `Qwen/Qwen2.5-VL-2B-Instruct` (vision, only if images present)\n",
    "**Web search:** SerpAPI only (requires a free key from https://serpapi.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73399052",
   "metadata": {},
   "source": [
    "## Step 1: Clone the Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone only if not already cloned\n",
    "if not os.path.exists('/content/HMRAG'):\n",
    "    !git clone https://github.com/ab-2109/HMRAG.git /content/HMRAG\n",
    "    print(\"‚úì Repository cloned\")\n",
    "else:\n",
    "    print(\"‚úì Repository already exists\")\n",
    "\n",
    "%cd /content/HMRAG\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c6b3b1",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies\n",
    "Installs all packages from `requirements.txt`. This handles SerpAPI (`google-search-results`), LightRAG, LangChain, transformers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60cc31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/HMRAG')\n",
    "\n",
    "print(\"Installing dependencies from requirements.txt (this may take a few minutes)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Install numpy first to avoid conflicts\n",
    "!pip install -q numpy==1.26.4\n",
    "\n",
    "# Install from the repo's requirements.txt\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Ensure critical packages are installed (in case requirements.txt missed any)\n",
    "!pip install -q google-search-results langchain-ollama huggingface_hub\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì All dependencies installed successfully!\")\n",
    "print(\"Note: Some dependency warnings are normal and won't affect functionality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7677c2",
   "metadata": {},
   "source": [
    "## Step 3: Patch Source Files (Critical Fixes)\n",
    "This cell patches the cloned source files to:\n",
    "1. **Fix embedding dimension mismatch**: `nomic-embed-text` outputs 768 dims, but LightRAG's `ollama_embed` decorator defaults to 1024. We use `ollama_embed.func` (unwrapped) and set `embedding_dim=768`.\n",
    "2. **Use `qwen2.5:1.5b`** everywhere instead of `qwen2.5:7b` (fits in Colab GPU memory).\n",
    "3. **Reduce `num_ctx` to 4096** (the 1.5B model can't handle 65536).\n",
    "4. **Use `Qwen2.5-VL-2B-Instruct`** for vision instead of the 7B variant.\n",
    "5. **Add HF token support** for downloading gated models.\n",
    "6. **Fix device handling** in the vision model to avoid dimension mismatches on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b9bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/HMRAG')\n",
    "\n",
    "# =============================================================================\n",
    "# PATCH 1: retrieval/vector_retrieval.py\n",
    "# =============================================================================\n",
    "with open('retrieval/vector_retrieval.py', 'w') as f:\n",
    "    f.write(\n",
    "        \"from lightrag import LightRAG, QueryParam\\n\"\n",
    "        \"from lightrag.llm.ollama import ollama_model_complete, ollama_embed\\n\"\n",
    "        \"from lightrag.utils import EmbeddingFunc\\n\"\n",
    "        \"\\n\"\n",
    "        \"from retrieval.base_retrieval import BaseRetrieval\\n\"\n",
    "        \"\\n\"\n",
    "        \"\\n\"\n",
    "        \"class VectorRetrieval(BaseRetrieval):\\n\"\n",
    "        \"    def __init__(self, config):\\n\"\n",
    "        \"        self.config = config\\n\"\n",
    "        \"        self.mode = getattr(config, 'mode', 'naive')\\n\"\n",
    "        \"        self.top_k = getattr(config, 'top_k', 4)\\n\"\n",
    "        \"        ollama_host = getattr(config, 'ollama_base_url', 'http://localhost:11434')\\n\"\n",
    "        \"        model_name = getattr(config, 'llm_model_name', 'qwen2.5:1.5b')\\n\"\n",
    "        \"        working_dir = getattr(config, 'working_dir', './lightrag_workdir')\\n\"\n",
    "        \"\\n\"\n",
    "        \"        self.client = LightRAG(\\n\"\n",
    "        \"            working_dir=working_dir,\\n\"\n",
    "        \"            llm_model_func=ollama_model_complete,\\n\"\n",
    "        \"            llm_model_name=model_name,\\n\"\n",
    "        \"            llm_model_max_async=4,\\n\"\n",
    "        '            llm_model_kwargs={\"host\": ollama_host, \"options\": {\"num_ctx\": 4096}},\\n'\n",
    "        \"            embedding_func=EmbeddingFunc(\\n\"\n",
    "        \"                embedding_dim=768,\\n\"\n",
    "        \"                max_token_size=8192,\\n\"\n",
    "        \"                func=lambda texts: ollama_embed.func(\\n\"\n",
    "        '                    texts, embed_model=\"nomic-embed-text\", host=ollama_host\\n'\n",
    "        \"                ),\\n\"\n",
    "        \"            ),\\n\"\n",
    "        \"        )\\n\"\n",
    "        \"        self.results = []\\n\"\n",
    "        \"\\n\"\n",
    "        \"    def find_top_k(self, query):\\n\"\n",
    "        \"        try:\\n\"\n",
    "        \"            self.results = self.client.query(\\n\"\n",
    "        \"                query,\\n\"\n",
    "        '                param=QueryParam(mode=\"naive\", top_k=self.top_k)\\n'\n",
    "        \"            )\\n\"\n",
    "        \"        except Exception as e:\\n\"\n",
    "        '            print(f\"VectorRetrieval error: {e}\")\\n'\n",
    "        '            self.results = f\"Vector retrieval failed: {str(e)}\"\\n'\n",
    "        \"        return self.results\\n\"\n",
    "    )\n",
    "print(\"‚úì Patched retrieval/vector_retrieval.py\")\n",
    "\n",
    "# =============================================================================\n",
    "# PATCH 2: retrieval/graph_retrieval.py\n",
    "# =============================================================================\n",
    "with open('retrieval/graph_retrieval.py', 'w') as f:\n",
    "    f.write(\n",
    "        \"from lightrag import LightRAG, QueryParam\\n\"\n",
    "        \"from lightrag.llm.ollama import ollama_model_complete, ollama_embed\\n\"\n",
    "        \"from lightrag.utils import EmbeddingFunc\\n\"\n",
    "        \"\\n\"\n",
    "        \"from retrieval.base_retrieval import BaseRetrieval\\n\"\n",
    "        \"\\n\"\n",
    "        \"\\n\"\n",
    "        \"class GraphRetrieval(BaseRetrieval):\\n\"\n",
    "        \"    def __init__(self, config):\\n\"\n",
    "        \"        self.config = config\\n\"\n",
    "        \"        self.mode = getattr(config, 'mode', 'mix')\\n\"\n",
    "        \"        self.top_k = getattr(config, 'top_k', 4)\\n\"\n",
    "        \"        ollama_host = getattr(config, 'ollama_base_url', 'http://localhost:11434')\\n\"\n",
    "        \"        model_name = getattr(config, 'llm_model_name', 'qwen2.5:1.5b')\\n\"\n",
    "        \"        working_dir = getattr(config, 'working_dir', './lightrag_workdir')\\n\"\n",
    "        \"\\n\"\n",
    "        \"        self.client = LightRAG(\\n\"\n",
    "        \"            working_dir=working_dir,\\n\"\n",
    "        \"            llm_model_func=ollama_model_complete,\\n\"\n",
    "        \"            llm_model_name=model_name,\\n\"\n",
    "        \"            llm_model_max_async=4,\\n\"\n",
    "        '            llm_model_kwargs={\"host\": ollama_host, \"options\": {\"num_ctx\": 4096}},\\n'\n",
    "        \"            embedding_func=EmbeddingFunc(\\n\"\n",
    "        \"                embedding_dim=768,\\n\"\n",
    "        \"                max_token_size=8192,\\n\"\n",
    "        \"                func=lambda texts: ollama_embed.func(\\n\"\n",
    "        '                    texts, embed_model=\"nomic-embed-text\", host=ollama_host\\n'\n",
    "        \"                ),\\n\"\n",
    "        \"            ),\\n\"\n",
    "        \"        )\\n\"\n",
    "        \"        self.results = []\\n\"\n",
    "        \"\\n\"\n",
    "        \"    def find_top_k(self, query):\\n\"\n",
    "        \"        try:\\n\"\n",
    "        \"            self.results = self.client.query(\\n\"\n",
    "        \"                query,\\n\"\n",
    "        \"                param=QueryParam(mode=self.mode, top_k=self.top_k)\\n\"\n",
    "        \"            )\\n\"\n",
    "        \"        except Exception as e:\\n\"\n",
    "        '            print(f\"GraphRetrieval error: {e}\")\\n'\n",
    "        '            self.results = f\"Graph retrieval failed: {str(e)}\"\\n'\n",
    "        \"        return self.results\\n\"\n",
    "    )\n",
    "print(\"‚úì Patched retrieval/graph_retrieval.py\")\n",
    "\n",
    "# =============================================================================\n",
    "# PATCH 3: retrieval/web_retrieval.py ‚Äî SerpAPI only, qwen2.5:1.5b\n",
    "# =============================================================================\n",
    "with open('retrieval/web_retrieval.py', 'w') as f:\n",
    "    f.write(\n",
    "        \"from langchain_community.utilities import SerpAPIWrapper\\n\"\n",
    "        \"from langchain_ollama import OllamaLLM\\n\"\n",
    "        \"\\n\"\n",
    "        \"from retrieval.base_retrieval import BaseRetrieval\\n\"\n",
    "        \"\\n\"\n",
    "        \"\\n\"\n",
    "        \"class WebRetrieval(BaseRetrieval):\\n\"\n",
    "        \"    def __init__(self, config):\\n\"\n",
    "        \"        self.config = config\\n\"\n",
    "        '        self.search_engine = \"Google\"\\n'\n",
    "        \"\\n\"\n",
    "        \"        serpapi_api_key = getattr(config, 'serpapi_api_key', '')\\n\"\n",
    "        \"        self.top_k = getattr(config, 'top_k', 4)\\n\"\n",
    "        \"        ollama_base_url = getattr(config, 'ollama_base_url', 'http://localhost:11434')\\n\"\n",
    "        \"        web_llm_model = getattr(config, 'web_llm_model_name', 'qwen2.5:1.5b')\\n\"\n",
    "        \"\\n\"\n",
    "        \"        self.client = SerpAPIWrapper(\\n\"\n",
    "        \"            serpapi_api_key=serpapi_api_key\\n\"\n",
    "        \"        )\\n\"\n",
    "        \"\\n\"\n",
    "        \"        self.llm = OllamaLLM(\\n\"\n",
    "        \"            base_url=ollama_base_url,\\n\"\n",
    "        \"            model=web_llm_model,\\n\"\n",
    "        \"            temperature=0.35,\\n\"\n",
    "        \"        )\\n\"\n",
    "        \"        self.results = []\\n\"\n",
    "        \"\\n\"\n",
    "        \"    def format_results(self, results):\\n\"\n",
    "        '        \"\"\"Format the search results into readable text.\"\"\"\\n'\n",
    "        \"        max_results = self.top_k\\n\"\n",
    "        \"        processed = []\\n\"\n",
    "        \"\\n\"\n",
    "        \"        if isinstance(results, dict):\\n\"\n",
    "        \"            if 'answerBox' in results:\\n\"\n",
    "        \"                answer = results['answerBox']\\n\"\n",
    "        \"                processed.append(\\n\"\n",
    "        \"                    f\\\"Direct answer: {answer.get('answer', '')}\\\\n\\\"\\n\"\n",
    "        \"                    f\\\"Source: {answer.get('link', '')}\\\\n\\\"\\n\"\n",
    "        \"                )\\n\"\n",
    "        \"\\n\"\n",
    "        \"            if 'organic' in results:\\n\"\n",
    "        \"                for item in results['organic'][:max_results]:\\n\"\n",
    "        \"                    processed.append(\\n\"\n",
    "        \"                        f\\\"[{item.get('title', 'No title')}]\\\\n\\\"\\n\"\n",
    "        \"                        f\\\"{item.get('snippet', 'No snippet')}\\\\n\\\"\\n\"\n",
    "        \"                        f\\\"Link: {item.get('link', '')}\\\\n\\\"\\n\"\n",
    "        \"                    )\\n\"\n",
    "        \"\\n\"\n",
    "        '        return \"\\\\n\".join(processed) if processed else \"No relevant results found\"\\n'\n",
    "        \"\\n\"\n",
    "        \"    def generation(self, results_with_query):\\n\"\n",
    "        '        \"\"\"Use Ollama model to generate an answer from search results.\"\"\"\\n'\n",
    "        \"        try:\\n\"\n",
    "        \"            answer = self.llm.invoke(results_with_query)\\n\"\n",
    "        \"        except Exception as e:\\n\"\n",
    "        '            print(f\"WebRetrieval generation error: {e}\")\\n'\n",
    "        '            answer = f\"Web generation failed: {str(e)}\"\\n'\n",
    "        \"        return answer\\n\"\n",
    "        \"\\n\"\n",
    "        \"    def find_top_k(self, query):\\n\"\n",
    "        \"        try:\\n\"\n",
    "        \"            raw_results = self.client.results(query)\\n\"\n",
    "        \"            formatted_results = self.format_results(raw_results)\\n\"\n",
    "        '            self.results = self.generation(formatted_results + \"\\\\n\" + query)\\n'\n",
    "        \"        except Exception as e:\\n\"\n",
    "        '            print(f\"WebRetrieval error: {e}\")\\n'\n",
    "        '            self.results = f\"Web retrieval failed: {str(e)}\"\\n'\n",
    "        \"        return self.results\\n\"\n",
    "    )\n",
    "print(\"‚úì Patched retrieval/web_retrieval.py\")\n",
    "\n",
    "# =============================================================================\n",
    "# PATCH 4: agents/decompose_agent.py ‚Äî qwen2.5:1.5b\n",
    "# =============================================================================\n",
    "with open('agents/decompose_agent.py', 'w') as f:\n",
    "    f.write(\n",
    "        \"import os\\n\"\n",
    "        \"import re\\n\"\n",
    "        \"from typing import List\\n\"\n",
    "        \"from langchain_core.prompts import PromptTemplate\\n\"\n",
    "        \"from langchain_ollama import OllamaLLM\\n\"\n",
    "        \"\\n\"\n",
    "        \"\\n\"\n",
    "        \"class DecomposeAgent:\\n\"\n",
    "        \"    def __init__(self, config):\\n\"\n",
    "        \"        self.config = config\\n\"\n",
    "        \"        self.llm = OllamaLLM(\\n\"\n",
    "        \"            base_url=getattr(config, 'ollama_base_url', 'http://localhost:11434'),\\n\"\n",
    "        \"            model=getattr(config, 'llm_model_name', 'qwen2.5:1.5b'),\\n\"\n",
    "        \"            temperature=getattr(config, 'temperature', 0.35),\\n\"\n",
    "        \"        )\\n\"\n",
    "        \"\\n\"\n",
    "        \"    def count_intents(self, query: str) -> int:\\n\"\n",
    "        \"        prompt = PromptTemplate.from_template(\\n\"\n",
    "        '            \"Please calculate how many independent intents are contained in the following query. \"\\n'\n",
    "        '            \"Return only an integer:\\\\n{query}\\\\nNumber of intents: \"\\n'\n",
    "        \"        )\\n\"\n",
    "        \"        max_attempts = 3\\n\"\n",
    "        \"        for attempt in range(max_attempts):\\n\"\n",
    "        \"            formatted_prompt = prompt.format(query=query)\\n\"\n",
    "        \"            response = self.llm.invoke(formatted_prompt)\\n\"\n",
    "        \"            try:\\n\"\n",
    "        \"                numbers = re.findall(r'\\\\d+', response.strip())\\n\"\n",
    "        \"                if numbers:\\n\"\n",
    "        \"                    return int(numbers[0])\\n\"\n",
    "        \"            except (ValueError, IndexError):\\n\"\n",
    "        \"                pass\\n\"\n",
    "        \"            if attempt == max_attempts - 1:\\n\"\n",
    "        \"                return 1\\n\"\n",
    "        \"        return 1\\n\"\n",
    "        \"\\n\"\n",
    "        \"    def decompose(self, query: str) -> List[str]:\\n\"\n",
    "        \"        intent_count = self.count_intents(query)\\n\"\n",
    "        \"        intent_count = min(intent_count, 3)\\n\"\n",
    "        \"        if intent_count > 1:\\n\"\n",
    "        \"            return self._split_query(query)\\n\"\n",
    "        \"        return [query]\\n\"\n",
    "        \"\\n\"\n",
    "        \"    def _split_query(self, query: str) -> List[str]:\\n\"\n",
    "        \"        prompt = PromptTemplate.from_template(\\n\"\n",
    "        '            \"Split the following query into multiple independent sub-queries, \"\\n'\n",
    "        \"            \\\"separated by '||', without additional explanations:\\\\n{query}\\\\nList of sub-queries: \\\"\\n\"\n",
    "        \"        )\\n\"\n",
    "        \"        formatted_prompt = prompt.format(query=query)\\n\"\n",
    "        \"        response = self.llm.invoke(formatted_prompt)\\n\"\n",
    "        '        sub_queries = [q.strip() for q in response.split(\"||\") if q.strip()]\\n'\n",
    "        \"        if not sub_queries:\\n\"\n",
    "        \"            return [query]\\n\"\n",
    "        \"        return sub_queries\\n\"\n",
    "    )\n",
    "print(\"‚úì Patched agents/decompose_agent.py\")\n",
    "\n",
    "# =============================================================================\n",
    "# PATCH 5: agents/summary_agent.py\n",
    "# =============================================================================\n",
    "with open('agents/summary_agent.py', 'w') as f:\n",
    "    f.write(\n",
    "        \"from collections import Counter\\n\"\n",
    "        \"from langchain_ollama import OllamaLLM\\n\"\n",
    "        \"import re\\n\"\n",
    "        \"from transformers import AutoProcessor\\n\"\n",
    "        \"import random\\n\"\n",
    "        \"import os\\n\"\n",
    "        \"import torch\\n\"\n",
    "        \"\\n\"\n",
    "        \"from prompts.base_prompt import build_prompt\\n\"\n",
    "        \"\\n\"\n",
    "        \"\\n\"\n",
    "        \"class SummaryAgent:\\n\"\n",
    "        \"    def __init__(self, config):\\n\"\n",
    "        \"        self.config = config\\n\"\n",
    "        \"        self.text_llm = OllamaLLM(\\n\"\n",
    "        \"            base_url=getattr(config, 'ollama_base_url', 'http://localhost:11434'),\\n\"\n",
    "        \"            model=getattr(config, 'llm_model_name', 'qwen2.5:1.5b')\\n\"\n",
    "        \"        )\\n\"\n",
    "        \"        self.hf_token = getattr(config, 'hf_token', '') or os.environ.get('HF_TOKEN', '')\\n\"\n",
    "        \"        self._vision_model = None\\n\"\n",
    "        \"        self._processor = None\\n\"\n",
    "        \"\\n\"\n",
    "        \"    def _load_vision_model(self):\\n\"\n",
    "        \"        if self._vision_model is None:\\n\"\n",
    "        \"            try:\\n\"\n",
    "        \"                from transformers import Qwen2_5_VLForConditionalGeneration\\n\"\n",
    "        \"\\n\"\n",
    "        '                model_name = \"Qwen/Qwen2.5-VL-2B-Instruct\"\\n'\n",
    "        \"\\n\"\n",
    "        \"                token_kwargs = {}\\n\"\n",
    "        \"                if self.hf_token:\\n\"\n",
    "        \"                    token_kwargs['token'] = self.hf_token\\n\"\n",
    "        \"\\n\"\n",
    "        \"                self._processor = AutoProcessor.from_pretrained(\\n\"\n",
    "        \"                    model_name, use_fast=True, **token_kwargs\\n\"\n",
    "        \"                )\\n\"\n",
    "        \"                self._vision_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\\n\"\n",
    "        \"                    model_name,\\n\"\n",
    "        \"                    torch_dtype=torch.float16,\\n\"\n",
    "        '                    device_map=\"auto\",\\n'\n",
    "        \"                    **token_kwargs\\n\"\n",
    "        \"                )\\n\"\n",
    "        \"            except Exception as e:\\n\"\n",
    "        '                print(f\"Warning: Could not load vision model: {e}\")\\n'\n",
    "        \"                self._vision_model = None\\n\"\n",
    "        \"                self._processor = None\\n\"\n",
    "        \"\\n\"\n",
    "        \"    def summarize(self, problems, shot_qids, qid, cur_ans):\\n\"\n",
    "        \"        problem = problems[qid]\\n\"\n",
    "        \"        question = problem['question']\\n\"\n",
    "        '        choices = problem[\"choices\"]\\n'\n",
    "        \"        answer = problem['answer']\\n\"\n",
    "        \"        image = problem.get('image', '')\\n\"\n",
    "        \"        caption = problem.get('caption', '')\\n\"\n",
    "        '        split = problem.get(\"split\", \"test\")\\n'\n",
    "        \"\\n\"\n",
    "        \"        most_ans = self.get_most_common_answer(cur_ans)\\n\"\n",
    "        \"\\n\"\n",
    "        \"        if len(most_ans) == 1:\\n\"\n",
    "        \"            prediction = self.get_result(most_ans[0])\\n\"\n",
    "        \"            pred_idx = self.get_pred_idx(prediction, choices, self.config.options)\\n\"\n",
    "        \"        else:\\n\"\n",
    "        '            if image and image == \"image.png\":\\n'\n",
    "        \"                image_path = os.path.join(self.config.image_root, split, qid, image)\\n\"\n",
    "        \"            else:\\n\"\n",
    "        '                image_path = \"\"\\n'\n",
    "        \"\\n\"\n",
    "        '            output_text = cur_ans[0] if len(cur_ans) > 0 else \"\"\\n'\n",
    "        '            output_graph = cur_ans[1] if len(cur_ans) > 1 else \"\"\\n'\n",
    "        '            output_web = cur_ans[2] if len(cur_ans) > 2 else \"\"\\n'\n",
    "        \"\\n\"\n",
    "        \"            output = self.refine(output_text, output_graph, output_web,\\n\"\n",
    "        \"                                 problems, shot_qids, qid, self.config, image_path)\\n\"\n",
    "        \"            if output is None:\\n\"\n",
    "        '                output = \"FAILED\"\\n'\n",
    "        '            print(f\"output: {output}\")\\n'\n",
    "        \"\\n\"\n",
    "        \"            ans_fusion = self.get_result(output)\\n\"\n",
    "        \"            pred_idx = self.get_pred_idx(ans_fusion, choices, self.config.options)\\n\"\n",
    "        \"        return pred_idx, cur_ans\\n\"\n",
    "        \"\\n\"\n",
    "        \"    def get_most_common_answer(self, res):\\n\"\n",
    "        \"        if not res:\\n\"\n",
    "        \"            return []\\n\"\n",
    "        \"        counter = Counter(res)\\n\"\n",
    "        \"        max_count = max(counter.values())\\n\"\n",
    "        \"        most_common_values = [item for item, count in counter.items() if count == max_count]\\n\"\n",
    "        \"        return most_common_values\\n\"\n",
    "        \"\\n\"\n",
    "        \"    def refine(self, output_text, output_graph, output_web, problems, shot_qids, qid, args, image_path):\\n\"\n",
    "        \"        prompt = build_prompt(problems, shot_qids, qid, args)\\n\"\n",
    "        '        prompt = f\"{prompt} The answer is A, B, C, D, E or FAILED. \\\\n BECAUSE: \"\\n'\n",
    "        \"\\n\"\n",
    "        \"        if not image_path:\\n\"\n",
    "        \"            output = self.text_llm.invoke(prompt)\\n\"\n",
    "        \"        else:\\n\"\n",
    "        \"            output = self.qwen_reasoning(prompt, image_path)\\n\"\n",
    "        \"            if output:\\n\"\n",
    "        '                print(f\"**** output: {output}\")\\n'\n",
    "        \"                output = self.text_llm.invoke(\\n\"\n",
    "        '                    f\"{output[0]} Summary the above information with format \"\\n'\n",
    "        \"                    f\\\"'Answer: The answer is A, B, C, D, E or FAILED.\\\\n BECAUSE: '\\\"\\n\"\n",
    "        \"                )\\n\"\n",
    "        \"            else:\\n\"\n",
    "        \"                output = self.text_llm.invoke(prompt)\\n\"\n",
    "        \"        return output\\n\"\n",
    "        \"\\n\"\n",
    "        \"    def get_result(self, output):\\n\"\n",
    "        \"        pattern = re.compile(r'The answer is ([A-E])')\\n\"\n",
    "        \"        res = pattern.findall(output)\\n\"\n",
    "        \"        if len(res) == 1:\\n\"\n",
    "        \"            answer = res[0]\\n\"\n",
    "        \"        else:\\n\"\n",
    "        '            answer = \"FAILED\"\\n'\n",
    "        \"        return answer\\n\"\n",
    "        \"\\n\"\n",
    "        \"    def get_pred_idx(self, prediction, choices, options):\\n\"\n",
    "        \"        if prediction in options[:len(choices)]:\\n\"\n",
    "        \"            return options.index(prediction)\\n\"\n",
    "        \"        else:\\n\"\n",
    "        \"            return random.choice(range(len(choices)))\\n\"\n",
    "        \"\\n\"\n",
    "        \"    def qwen_reasoning(self, prompt, image_path):\\n\"\n",
    "        \"        self._load_vision_model()\\n\"\n",
    "        \"        if self._vision_model is None or self._processor is None:\\n\"\n",
    "        '            print(\"Warning: Vision model not available, falling back to text-only.\")\\n'\n",
    "        \"            return None\\n\"\n",
    "        \"\\n\"\n",
    "        \"        try:\\n\"\n",
    "        \"            from qwen_vl_utils import process_vision_info\\n\"\n",
    "        \"        except ImportError:\\n\"\n",
    "        '            print(\"Warning: qwen_vl_utils not installed, falling back to text-only.\")\\n'\n",
    "        \"            return None\\n\"\n",
    "        \"\\n\"\n",
    "        \"        messages = [\\n\"\n",
    "        \"            {\\n\"\n",
    "        '                \"role\": \"user\",\\n'\n",
    "        '                \"content\": [\\n'\n",
    "        \"                    {\\n\"\n",
    "        '                        \"type\": \"image\",\\n'\n",
    "        '                        \"image\": image_path,\\n'\n",
    "        \"                    },\\n\"\n",
    "        '                    {\"type\": \"text\", \"text\": prompt},\\n'\n",
    "        \"                ],\\n\"\n",
    "        \"            }\\n\"\n",
    "        \"        ]\\n\"\n",
    "        \"\\n\"\n",
    "        \"        text = self._processor.apply_chat_template(\\n\"\n",
    "        \"            messages, tokenize=False, add_generation_prompt=True\\n\"\n",
    "        \"        )\\n\"\n",
    "        \"        image_inputs, video_inputs = process_vision_info(messages)\\n\"\n",
    "        \"        inputs = self._processor(\\n\"\n",
    "        \"            text=[text],\\n\"\n",
    "        \"            images=image_inputs,\\n\"\n",
    "        \"            videos=video_inputs,\\n\"\n",
    "        \"            padding=True,\\n\"\n",
    "        '            return_tensors=\"pt\",\\n'\n",
    "        \"        )\\n\"\n",
    "        \"\\n\"\n",
    "        \"        device = next(self._vision_model.parameters()).device\\n\"\n",
    "        \"        inputs = {k: v.to(device) if hasattr(v, 'to') else v for k, v in inputs.items()}\\n\"\n",
    "        \"\\n\"\n",
    "        \"        generated_ids = self._vision_model.generate(**inputs, max_new_tokens=512)\\n\"\n",
    "        \"        generated_ids_trimmed = [\\n\"\n",
    "        \"            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs['input_ids'], generated_ids)\\n\"\n",
    "        \"        ]\\n\"\n",
    "        \"        output_text = self._processor.batch_decode(\\n\"\n",
    "        \"            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\\n\"\n",
    "        \"        )\\n\"\n",
    "        \"        return output_text\\n\"\n",
    "    )\n",
    "print(\"‚úì Patched agents/summary_agent.py\")\n",
    "\n",
    "# =============================================================================\n",
    "# PATCH 6: main.py ‚Äî qwen2.5:1.5b default, add --hf_token argument\n",
    "# =============================================================================\n",
    "with open('main.py', 'r') as f:\n",
    "    main_content = f.read()\n",
    "\n",
    "changes_made = []\n",
    "\n",
    "if \"default='qwen2.5:7b'\" in main_content:\n",
    "    main_content = main_content.replace(\"default='qwen2.5:7b'\", \"default='qwen2.5:1.5b'\")\n",
    "    changes_made.append(\"qwen2.5:7b -> qwen2.5:1.5b\")\n",
    "\n",
    "if '--hf_token' not in main_content:\n",
    "    main_content = main_content.replace(\n",
    "        \"parser.add_argument('--top_k', type=int, default=4)\\n\",\n",
    "        \"parser.add_argument('--top_k', type=int, default=4)\\n\"\n",
    "        \"    parser.add_argument('--hf_token', type=str, default='',\\n\"\n",
    "        \"                        help='Hugging Face access token for downloading gated models')\\n\"\n",
    "    )\n",
    "    changes_made.append(\"Added --hf_token argument\")\n",
    "\n",
    "if 'HF_TOKEN' not in main_content:\n",
    "    main_content = main_content.replace(\n",
    "        \"    agent = MRetrievalAgent(args)\",\n",
    "        \"    # Set HF token if provided\\n\"\n",
    "        \"    if args.hf_token:\\n\"\n",
    "        \"        import os as _os\\n\"\n",
    "        \"        _os.environ['HF_TOKEN'] = args.hf_token\\n\"\n",
    "        \"        _os.environ['HUGGING_FACE_HUB_TOKEN'] = args.hf_token\\n\"\n",
    "        \"        try:\\n\"\n",
    "        \"            from huggingface_hub import login\\n\"\n",
    "        \"            login(token=args.hf_token)\\n\"\n",
    "        \"            print('Logged in to Hugging Face Hub')\\n\"\n",
    "        \"        except Exception as e:\\n\"\n",
    "        \"            print(f'Warning: Could not login to HF Hub: {e}')\\n\"\n",
    "        \"\\n\"\n",
    "        \"    agent = MRetrievalAgent(args)\"\n",
    "    )\n",
    "    changes_made.append(\"Added HF login logic\")\n",
    "\n",
    "with open('main.py', 'w') as f:\n",
    "    f.write(main_content)\n",
    "\n",
    "if changes_made:\n",
    "    print(\"‚úì Patched main.py: \" + \", \".join(changes_made))\n",
    "else:\n",
    "    print(\"‚úì main.py already up to date\")\n",
    "\n",
    "# =============================================================================\n",
    "# PATCH 7: YAML config files\n",
    "# =============================================================================\n",
    "for yaml_file in ['configs/decompose_agent.yaml', 'configs/multi_retrieval_agents.yaml']:\n",
    "    if os.path.exists(yaml_file):\n",
    "        with open(yaml_file, 'r') as f:\n",
    "            content = f.read()\n",
    "        if 'qwen2.5:7b' in content:\n",
    "            content = content.replace('qwen2.5:7b', 'qwen2.5:1.5b')\n",
    "            with open(yaml_file, 'w') as f:\n",
    "                f.write(content)\n",
    "            print(f\"‚úì Patched {yaml_file}: qwen2.5:7b -> qwen2.5:1.5b\")\n",
    "        else:\n",
    "            print(f\"‚úì {yaml_file} already correct\")\n",
    "\n",
    "# Clean up stale working directory\n",
    "!rm -rf ./lightrag_workdir\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì All patches applied!\")\n",
    "print(\"  - embedding_dim = 768 (matches nomic-embed-text)\")\n",
    "print(\"  - ollama_embed.func (bypasses 1024-dim decorator)\")\n",
    "print(\"  - num_ctx = 4096 (fits qwen2.5:1.5b)\")\n",
    "print(\"  - Text model: qwen2.5:1.5b\")\n",
    "print(\"  - Vision model: Qwen/Qwen2.5-VL-2B-Instruct\")\n",
    "print(\"  - Web search: SerpAPI only\")\n",
    "print(\"  - HF token support added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be45fc17",
   "metadata": {},
   "source": [
    "## Step 4: Install and Start Ollama + Pull Models\n",
    "Ollama runs locally on the Colab VM. We pull `qwen2.5:1.5b` (~1GB) and `nomic-embed-text` (~270MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Install system dependencies\n",
    "print(\"Installing system dependencies...\")\n",
    "!sudo apt-get update -qq 2>/dev/null\n",
    "!sudo apt-get install -y -qq zstd 2>/dev/null\n",
    "\n",
    "# Install Ollama\n",
    "print(\"Installing Ollama...\")\n",
    "!curl -fsSL https://ollama.com/install.sh | sh 2>&1 | tail -3\n",
    "\n",
    "# Find ollama binary\n",
    "result = subprocess.run(['which', 'ollama'], capture_output=True, text=True)\n",
    "ollama_path = result.stdout.strip()\n",
    "if not ollama_path:\n",
    "    for path in ['/usr/local/bin/ollama', '/usr/bin/ollama']:\n",
    "        if os.path.exists(path):\n",
    "            ollama_path = path\n",
    "            break\n",
    "\n",
    "if not ollama_path:\n",
    "    print(\"‚ùå Ollama binary not found! Please restart runtime and try again.\")\n",
    "else:\n",
    "    print(f\"‚úì Ollama found at: {ollama_path}\")\n",
    "    \n",
    "    # Kill any existing ollama processes\n",
    "    subprocess.run(['pkill', '-f', 'ollama'], stderr=subprocess.DEVNULL)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Start Ollama server in background\n",
    "    print(\"Starting Ollama server...\")\n",
    "    ollama_process = subprocess.Popen(\n",
    "        [ollama_path, 'serve'],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    time.sleep(8)\n",
    "    \n",
    "    # Verify server is running\n",
    "    result = subprocess.run(['curl', '-s', 'http://localhost:11434/api/tags'],\n",
    "                          capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úì Ollama server is running!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Ollama server may not be ready yet. Waiting more...\")\n",
    "        time.sleep(10)\n",
    "    \n",
    "    # Pull the text model\n",
    "    print(\"\\nPulling qwen2.5:1.5b model (~1GB, may take 2-5 min)...\")\n",
    "    !{ollama_path} pull qwen2.5:1.5b\n",
    "    \n",
    "    # Pull the embedding model\n",
    "    print(\"\\nPulling nomic-embed-text model (~270MB)...\")\n",
    "    !{ollama_path} pull nomic-embed-text\n",
    "    \n",
    "    print(\"\\n‚úì Ollama setup complete!\")\n",
    "    print(\"\\nAvailable models:\")\n",
    "    !{ollama_path} list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0371b5",
   "metadata": {},
   "source": [
    "## Step 5: Configure API Keys\n",
    "- **SerpAPI key** (required for web search): Get a free key at https://serpapi.com\n",
    "- **HF token** (optional, for gated models): Get from https://huggingface.co/settings/tokens\n",
    "\n",
    "You can set them via Colab's **Secrets** (left sidebar ‚Üí üîë icon) or paste directly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea63e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# =====================================================\n",
    "# OPTION 1: Use Colab Secrets (recommended)\n",
    "# Add SERPAPI_API_KEY and HF_TOKEN in left sidebar ‚Üí üîë\n",
    "# =====================================================\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    SERPAPI_API_KEY = userdata.get('SERPAPI_API_KEY')\n",
    "    print(\"‚úì SERPAPI_API_KEY loaded from Colab Secrets\")\n",
    "except Exception:\n",
    "    # OPTION 2: Paste your key directly here\n",
    "    SERPAPI_API_KEY = \"\"  # <-- PASTE YOUR SERPAPI KEY HERE\n",
    "    if SERPAPI_API_KEY:\n",
    "        print(\"‚úì SERPAPI_API_KEY set manually\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è SERPAPI_API_KEY not set! Web search will fail.\")\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"‚úì HF_TOKEN loaded from Colab Secrets\")\n",
    "except Exception:\n",
    "    # OPTION 2: Paste your HF token directly here\n",
    "    HF_TOKEN = \"\"  # <-- PASTE YOUR HF TOKEN HERE (optional)\n",
    "    if HF_TOKEN:\n",
    "        print(\"‚úì HF_TOKEN set manually\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è HF_TOKEN not set (optional ‚Äî only needed for gated models)\")\n",
    "\n",
    "# Store in environment for the subprocess calls\n",
    "os.environ['SERPAPI_API_KEY'] = SERPAPI_API_KEY or ''\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN or ''\n",
    "\n",
    "print(\"\\nAPI keys configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ca8a8d",
   "metadata": {},
   "source": [
    "## Step 6: Download ScienceQA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f932419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/HMRAG')\n",
    "\n",
    "# Create dataset directory\n",
    "os.makedirs('dataset', exist_ok=True)\n",
    "os.chdir('dataset')\n",
    "\n",
    "# Clone the ScienceQA repository\n",
    "if not os.path.exists('ScienceQA'):\n",
    "    print(\"Cloning ScienceQA repository...\")\n",
    "    !git clone https://github.com/lupantech/ScienceQA\n",
    "else:\n",
    "    print(\"‚úì ScienceQA directory already exists\")\n",
    "\n",
    "if os.path.exists('ScienceQA'):\n",
    "    os.chdir('ScienceQA')\n",
    "    \n",
    "    # Download the dataset\n",
    "    if os.path.exists('tools/download.sh'):\n",
    "        print(\"Downloading dataset files (this may take several minutes)...\")\n",
    "        !bash tools/download.sh\n",
    "    else:\n",
    "        print(\"download.sh not found, creating data directory...\")\n",
    "        os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    os.chdir('/content/HMRAG')\n",
    "\n",
    "# Verify dataset structure\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Checking required files:\")\n",
    "required_files = [\n",
    "    'dataset/ScienceQA/data/scienceqa/problems.json',\n",
    "    'dataset/ScienceQA/data/scienceqa/pid_splits.json'\n",
    "]\n",
    "\n",
    "# Also check alternative locations\n",
    "alt_files = [\n",
    "    'dataset/ScienceQA/data/problems.json',\n",
    "    'dataset/ScienceQA/data/pid_splits.json'\n",
    "]\n",
    "\n",
    "data_root = None\n",
    "for f in required_files:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"‚úì Found: {f}\")\n",
    "        if 'problems.json' in f:\n",
    "            data_root = os.path.dirname(f)\n",
    "    else:\n",
    "        print(f\"  Not at: {f}\")\n",
    "\n",
    "if data_root is None:\n",
    "    for f in alt_files:\n",
    "        if os.path.exists(f):\n",
    "            print(f\"‚úì Found: {f}\")\n",
    "            if 'problems.json' in f:\n",
    "                data_root = os.path.dirname(f)\n",
    "        else:\n",
    "            print(f\"  Not at: {f}\")\n",
    "\n",
    "if data_root:\n",
    "    print(f\"\\n‚úì Data root: {data_root}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Could not find problems.json automatically.\")\n",
    "    print(\"Please check the dataset structure manually:\")\n",
    "    !find dataset/ScienceQA -name \"problems.json\" 2>/dev/null | head -5\n",
    "    print(\"\\nYou'll need to set --data_root accordingly in the run command.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e0202",
   "metadata": {},
   "source": [
    "## Step 7: Verify Everything Before Running\n",
    "Quick check that Ollama server is running and models are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "os.chdir('/content/HMRAG')\n",
    "\n",
    "# Check Ollama server\n",
    "print(\"=\" * 50)\n",
    "print(\"CHECKING OLLAMA SERVER\")\n",
    "print(\"=\" * 50)\n",
    "try:\n",
    "    result = subprocess.run(['curl', '-s', 'http://localhost:11434/api/tags'],\n",
    "                          capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úì Ollama server is running!\")\n",
    "    else:\n",
    "        raise Exception(\"Not responding\")\n",
    "except Exception:\n",
    "    print(\"‚ö†Ô∏è Ollama server not running. Restarting...\")\n",
    "    subprocess.run(['pkill', '-f', 'ollama'], stderr=subprocess.DEVNULL)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    result = subprocess.run(['which', 'ollama'], capture_output=True, text=True)\n",
    "    ollama_path = result.stdout.strip() or '/usr/local/bin/ollama'\n",
    "    \n",
    "    subprocess.Popen([ollama_path, 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    time.sleep(8)\n",
    "    \n",
    "    result = subprocess.run(['curl', '-s', 'http://localhost:11434/api/tags'],\n",
    "                          capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úì Ollama server restarted!\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to start Ollama. Restart runtime and rerun.\")\n",
    "\n",
    "print(\"\\nAvailable models:\")\n",
    "!ollama list\n",
    "\n",
    "# Check critical files\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CHECKING SOURCE FILES\")\n",
    "print(\"=\" * 50)\n",
    "critical_files = [\n",
    "    'main.py',\n",
    "    'agents/decompose_agent.py',\n",
    "    'agents/summary_agent.py',\n",
    "    'agents/multi_retrieval_agents.py',\n",
    "    'retrieval/vector_retrieval.py',\n",
    "    'retrieval/graph_retrieval.py',\n",
    "    'retrieval/web_retrieval.py',\n",
    "    'retrieval/base_retrieval.py',\n",
    "    'prompts/base_prompt.py',\n",
    "]\n",
    "for f in critical_files:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"‚úì {f}\")\n",
    "    else:\n",
    "        print(f\"‚ùå MISSING: {f}\")\n",
    "\n",
    "# Quick import test\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TESTING IMPORTS\")\n",
    "print(\"=\" * 50)\n",
    "try:\n",
    "    import sys\n",
    "    sys.path.insert(0, '/content/HMRAG')\n",
    "    from langchain_community.utilities import SerpAPIWrapper\n",
    "    print(\"‚úì SerpAPIWrapper (google-search-results)\")\n",
    "    from langchain_ollama import OllamaLLM\n",
    "    print(\"‚úì OllamaLLM (langchain-ollama)\")\n",
    "    from lightrag import LightRAG\n",
    "    print(\"‚úì LightRAG (lightrag-hku)\")\n",
    "    from lightrag.llm.ollama import ollama_model_complete, ollama_embed\n",
    "    print(\"‚úì ollama_model_complete, ollama_embed\")\n",
    "    print(\"\\n‚úì All imports successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Try rerunning Step 2 (Install Dependencies)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa9373",
   "metadata": {},
   "source": [
    "## Step 8: Run Inference ‚Äî Small Test (5 examples)\n",
    "**Important:** Always delete `./lightrag_workdir` before running to avoid stale dimension-mismatch errors from previous runs.\n",
    "\n",
    "Adjust `--data_root` below if your dataset location differs. Common paths:\n",
    "- `./dataset/ScienceQA/data/scienceqa`\n",
    "- `./dataset/ScienceQA/data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f502958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/HMRAG')\n",
    "\n",
    "# IMPORTANT: Clean lightrag_workdir to avoid dimension mismatch from old runs\n",
    "!rm -rf ./lightrag_workdir\n",
    "!mkdir -p outputs\n",
    "\n",
    "# Auto-detect data_root\n",
    "data_root = \"\"\n",
    "for candidate in [\n",
    "    \"./dataset/ScienceQA/data/scienceqa\",\n",
    "    \"./dataset/ScienceQA/data\",\n",
    "]:\n",
    "    if os.path.exists(os.path.join(candidate, \"problems.json\")):\n",
    "        data_root = candidate\n",
    "        break\n",
    "\n",
    "if not data_root:\n",
    "    print(\"‚ùå Could not find problems.json. Please set data_root manually.\")\n",
    "    print(\"Searching for it...\")\n",
    "    !find dataset/ -name \"problems.json\" 2>/dev/null\n",
    "else:\n",
    "    print(f\"Using data_root: {data_root}\")\n",
    "    \n",
    "    # Build the command\n",
    "    serpapi_key = os.environ.get('SERPAPI_API_KEY', '')\n",
    "    hf_token = os.environ.get('HF_TOKEN', '')\n",
    "    \n",
    "    cmd = f\"\"\"python3 main.py \\\n",
    "    --data_root {data_root} \\\n",
    "    --image_root ./dataset/ScienceQA/data/scienceqa \\\n",
    "    --output_root ./outputs \\\n",
    "    --working_dir ./lightrag_workdir \\\n",
    "    --serpapi_api_key \"{serpapi_key}\" \\\n",
    "    --llm_model_name qwen2.5:1.5b \\\n",
    "    --web_llm_model_name qwen2.5:1.5b \\\n",
    "    --test_split test \\\n",
    "    --test_number 5 \\\n",
    "    --shot_number 0 \\\n",
    "    --label test_run \\\n",
    "    --save_every 5\"\"\"\n",
    "    \n",
    "    if hf_token:\n",
    "        cmd += f' --hf_token \"{hf_token}\"'\n",
    "    \n",
    "    print(f\"\\nRunning command:\\n{cmd}\\n\")\n",
    "    !{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86214ea",
   "metadata": {},
   "source": [
    "## Step 9: Run Full Inference\n",
    "After the small test works, run on the full test set. This will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef117fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/HMRAG')\n",
    "\n",
    "# IMPORTANT: Clean lightrag_workdir to avoid dimension mismatch\n",
    "!rm -rf ./lightrag_workdir\n",
    "\n",
    "# Auto-detect data_root\n",
    "data_root = \"\"\n",
    "for candidate in [\n",
    "    \"./dataset/ScienceQA/data/scienceqa\",\n",
    "    \"./dataset/ScienceQA/data\",\n",
    "]:\n",
    "    if os.path.exists(os.path.join(candidate, \"problems.json\")):\n",
    "        data_root = candidate\n",
    "        break\n",
    "\n",
    "serpapi_key = os.environ.get('SERPAPI_API_KEY', '')\n",
    "hf_token = os.environ.get('HF_TOKEN', '')\n",
    "\n",
    "cmd = f\"\"\"python3 main.py \\\n",
    "    --data_root {data_root} \\\n",
    "    --image_root ./dataset/ScienceQA/data/scienceqa \\\n",
    "    --output_root ./outputs \\\n",
    "    --working_dir ./lightrag_workdir \\\n",
    "    --serpapi_api_key \"{serpapi_key}\" \\\n",
    "    --llm_model_name qwen2.5:1.5b \\\n",
    "    --web_llm_model_name qwen2.5:1.5b \\\n",
    "    --test_split test \\\n",
    "    --shot_number 2 \\\n",
    "    --label full_run \\\n",
    "    --save_every 50 \\\n",
    "    --use_caption\"\"\"\n",
    "\n",
    "if hf_token:\n",
    "    cmd += f' --hf_token \"{hf_token}\"'\n",
    "\n",
    "print(f\"Running command:\\n{cmd}\\n\")\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94fceda",
   "metadata": {},
   "source": [
    "## Step 10: View & Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73943b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "os.chdir('/content/HMRAG')\n",
    "\n",
    "# List output files\n",
    "print(\"Output files:\")\n",
    "!ls -lh outputs/\n",
    "\n",
    "# Load and display results\n",
    "output_files = sorted(glob.glob('outputs/*.json'))\n",
    "if output_files:\n",
    "    for fpath in output_files:\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(f\"File: {os.path.basename(fpath)}\")\n",
    "        with open(fpath, 'r') as f:\n",
    "            results = json.load(f)\n",
    "        print(f\"Total results: {len(results)}\")\n",
    "        print(\"Sample results:\")\n",
    "        for qid, answer in list(results.items())[:5]:\n",
    "            print(f\"  Question {qid}: Answer = {answer}\")\n",
    "else:\n",
    "    print(\"No output files found yet. Run inference first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91843237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results to your local machine\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "os.chdir('/content/HMRAG')\n",
    "\n",
    "# Zip all outputs\n",
    "!zip -r outputs.zip outputs/\n",
    "files.download('outputs.zip')\n",
    "print(\"‚úì Download started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f5088",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### \"Dimension mismatch\" / \"embedding_dim\" errors\n",
    "```\n",
    "rm -rf ./lightrag_workdir\n",
    "```\n",
    "Then rerun. This happens when a previous run created the DB with a different embedding dimension.\n",
    "\n",
    "### \"Ollama connection refused\"\n",
    "Re-run the Ollama setup cell (Step 4) or the verification cell (Step 7).\n",
    "\n",
    "### \"SerpAPI error\"\n",
    "Make sure your `SERPAPI_API_KEY` is set correctly in Step 5. Get a free key at https://serpapi.com\n",
    "\n",
    "### \"CUDA out of memory\"\n",
    "The notebook uses `qwen2.5:1.5b` (text) and `Qwen2.5-VL-2B-Instruct` (vision). If you still run out of memory:\n",
    "- Use `Runtime ‚Üí Change runtime type ‚Üí T4 GPU`\n",
    "- Restart runtime and rerun all cells"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
